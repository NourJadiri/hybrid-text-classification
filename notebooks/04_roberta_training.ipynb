{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e44df6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "# Set the current working directory to the project root\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad8a4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "narratives",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "subnarratives",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "language",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "narrative_ids",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "subnarrative_ids",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "num_narratives",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "num_subnarratives",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "word_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "word_count_bin",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "labels",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "980c467c-f5b0-45a3-b6a5-bd4a885affa8",
       "rows": [
        [
         "0",
         "BG_670.txt",
         "Опитът на колективния Запад да „обезкърви Русия“ с ръцете на властите в Киев „се провали с гръм и трясък“ и скоро от Украйна ...\n\nОпитът на колективния Запад да „обезкърви Русия“ с ръцете на властите в Киев „се провали с гръм и трясък“ и скоро от Украйна няма да остане почти нищо, ако не започне процесът на разрешаване на този въоръжен конфликт. Тази гледна точка изрази пред ТАСС бившият началник на кабинета на държавния секретар на САЩ Колин Пауъл, пенсионирания полковник от армията на САЩ Лорънс Уилкерсън.\n\"Подкрепата на САЩ, НАТО и други западни съюзници за войната в Украйна срещу Русия е безумна. Това води до смъртта на украински войници в името на загубена кауза, ако не вземете предвид печеленето на пари от американски и европейски военни изпълнители, както и бруталния опит да се обезкърви Русия чрез трети страни“, каза Уилкерсън.\n\"Усилията за постигане на тази последна цел, при цялата ѝ подлост, се провалиха с гръм и трясък\", заяви видният американски военен анализатор. - В действителност Русия очевидно побеждава\". Ако скоро не бъде обявено прекратяване на огъня и не бъде свикана истинска мирна конференция, тогава от Украйна практически нищо няма да остане“, убеден е Уилкерсън. Той спомена \"истинска мирна конференция\" в противовес на срещата в Бюргенсток (Швейцария), която беше свикана на 15-16 юни по инициатива на Запада и на която Русия не беше поканена. Дори много западни политически анализатори заявиха след срещата, че тя е претърпяла фиаско. Нито една страна от БРИКС не подкрепи заключителния документ от срещата.",
         "['URW: Blaming the war on others rather than the invader'\n 'URW: Discrediting the West, Diplomacy'\n 'URW: Discrediting the West, Diplomacy'\n 'URW: Amplifying war-related fears']",
         "['URW: Blaming the war on others rather than the invader: The West are the aggressors'\n 'URW: Discrediting the West, Diplomacy: Other'\n 'URW: Discrediting the West, Diplomacy: The West does not care about Ukraine, only about its interests'\n 'URW: Amplifying war-related fears: Other']",
         "BG",
         "[11, 12, 14]",
         "[88, 70, 74, 86]",
         "4",
         "4",
         "248",
         "101-250",
         "[0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0]"
        ],
        [
         "1",
         "A7_URW_BG_4793.txt",
         "Цончо Ганев, “Възраждане”: Обещали сме на Украйна боеприпаси, ракети, както и над 40 танка. Затова ли Радев отказа за срещата на НАТО\n\n“Възраждане” иска да информация каква е позицията на България за Срещата на върха на НАТО\nСпоред документи на украинската държава, България ще им подари над 40 танка, ракети и много военна техника\n„НАТО е всичко друго, но не и отбранителен съюз. Ние винаги сме били на позиция, че държавата ни няма място в какъвто и да е военен блок, винаги сме смятали, че България не трябва да е част от НАТО, но за да сме максимално демократични, нашата позиция е, че българският народ трябва да се произнесе и трябва да има референдум дали да сме член на НАТО или не.“, каза от трибуната на Народното събрание заместник-председателят на „Възраждане“ Цончо Ганев в декларация прочетена от парламентарната група.\nСпоред него, Съединените американски щати може и да провеждат мироопазващи военни операции, например в Сирия, Ирак, Афганистан и в ред други държави, но по съвсем консервативни данни през последните 70 години жертвите в техни мироопазващи военни операции са повече от 40 милиона души.\n„Ако вземем за пример специалната военна операция, ще видим, че от една страна е Русия, от друга - Украйна, която разчита на издръжката и подкрепата на НАТО, от което България също е част. За да е пълна подигравката, още когато започна тази специална военна операция, тогавашното правителство на Кирил Петков, Атанасов, Христо Иванов. Слави Трифонов и Корнелия Нинова, започна да продава оръжия през посредници до украинската граница. Стигнахме дотам, че следващите наши правителства вече подаряваха оръжие, включително и служебните правителства на Радев, които опразниха част от складовете ни с военна техника.“, коментира още Ганев.\nСпоред официална информация, която излиза, е видно, че към днешна дата този военен конфликт върви към развръзка и това вече не се крие дори от държавите членки на НАТО, а украинската държава губи все повече територии.\n„След вчерашните изявления на Зеленски става ясно, че войната трябва да се прекрати и да се направи това, което искат руските интереси в целия този многопластов проблем. Притеснителното е, че България е част от него. Повече от 4 години във Варна има команден център на НАТО и България вече е изправена пред угрозата да бъде атакувана, ако конфликтът се разрасне.“, допълни Ганев.\nТой припомни, че след няколко дни предстои Среща на върха на НАТО в САЩ, на която Президентът няма да присъства.\n„Видяхме официални документи на украинската държава, в които пише, че нашата страна е поела отговорност за изпращането и подаряването на военна техника, на снаряжение, на боеприпаси, на ракетите ни, както и над 40 танка. Каква е опасността пред държавата ни, че чак Президентът не иска да отиде на Срещата на върха? Това е нещото, на което трябва да отговори евроатлантическото правителство, защото касае нашата национална сигурност и ако пари се печелят, бюджет и приходи се изравняват и се възстановяват, от войната няма връщане назад. Защото цар Борис Трети е казал „по-добре да ядем черен хляб, отколкото майките и жените ни да носят черни забрадки.“, каза в заключение Цончо Ганев.",
         "['URW: Discrediting Ukraine']",
         "['URW: Discrediting Ukraine: Situation in Ukraine is hopeless']",
         "BG",
         "[13]",
         "[81]",
         "1",
         "1",
         "503",
         "501-1000",
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0]"
        ],
        [
         "2",
         "BG_3245.txt",
         "Подкрепата за Киев от страна на Запада вече не изглежда непоклатима\n\nВ деня на 80-ата годишнина от десанта на съюзниците в Нормандия конфликтът в Украйна продължава на фона на нестабилността на Западния фронт.\nНа тържествените мероприятия лидерите на западните страни ще демонстрират единен фронт: ще положат венци пред гробовете на загиналите и ще обещаят непоколебима подкрепа на Киев. Но настроенията на бойните полета в Украйна и мащабните промени в глобалната политика говорят друго.\nПодкрепата на Запада, който до тази година осигуряваше доставките на оръжие за Киев, вече не изглежда непоклатима.\nОбещанията за подкрепа и солидарност с Украйна може да се окажат празни, тъй като светът се подготвя за потенциални сеизмични промени. Европейците се опасяват, че ако тази есен Байдън загуби от Тръмп в борбата за Белия дом, Украйна може да загуби най-важния си съюзник.\nИ дори ако Джо Байдън бъде преизбран, не всички членове на неговата администрация ще искат да обединят сили с Украйна и да оглавят победоносно контранастъпление срещу Русия, заяви френският политолог Никола Тензер.\nМеждувременно в цяла Европа традиционните партии се опасяват, че крайната десница може да спечели голяма победа на започващите избори за Европейски парламент Тази седмица.",
         "['URW: Discrediting the West, Diplomacy' 'URW: Discrediting Ukraine'\n 'URW: Discrediting the West, Diplomacy']",
         "['URW: Discrediting the West, Diplomacy: The West is weak'\n 'URW: Discrediting Ukraine: Situation in Ukraine is hopeless'\n 'URW: Discrediting the West, Diplomacy: The EU is divided']",
         "BG",
         "[13, 14]",
         "[81, 90, 87]",
         "3",
         "3",
         "190",
         "101-250",
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0]"
        ],
        [
         "3",
         "A9_BG_5190.txt",
         "Дмитрий Медведев: НПО-та, спонсорирани от Сорос, са съучастници на терористите в Киев\n\nЗаместник-председателят на руския Съвет за сигурност Дмитрий Медведев заяви, че Международната федерация за правата на човека (FIDH), финансирана от фондация \"Отворено общество\" (призната за нежелана в Русия) на милиардера Джордж Сорос, която се обърна към Международния наказателен съд с искане за издаване на заповед за арест, е \"съучастник на терористите в Киев\", предават РИА Новости и \"Фокус\".\nМеждународната федерация за правата на човека на Сорос (FIDH) и външните министерства на редица западни държави отправят искане до прокуратурата на Международния наказателен съд за издаване на заповеди за \"арест\" на заместник-председателя на руския Съвет за сигурност и бивш президент и премиер на РФ Дмитрий Медведев, заместник-началника на администрацията на руския президент Алексей Громов и редица руски журналисти.\n\"Колко е приятно да получим признание за ефективността на съвместните ни усилия срещу неонацисткия режим в Киев! То дойде от гадни организации като неправителствените организации, собственост на отвратителния стар герой Сорос, който отиде в Международния наказателен съд заради нашата скромна работа. Такива НПО-та и техните господари са съучастници на терористите, които само вчера убиха повече от 20 наши цивилни граждани\", написа Медведев в социалната мрежа X.\nМедведев отбелязва, че това го е мотивирало да продължи работата си срещу \"подлата нацистка клика\".\nПо-рано директорът по правните въпроси на проекта Docket на фондация \"Клуни\" Анна Нейстат заяви, че организацията се стреми в Европа да бъдат издадени заповеди за арест на руски журналисти. Съоснователи на фондация \"Клуни\" са актьорът Джордж Клуни и съпругата му Амал.\nКакто самият Джордж Клуни обясни по-късно, фондацията \"се е изразила погрешно\" и организацията, която има \"дълга история на защита на журналисти, никога няма да ги преследва\".",
         "['URW: Discrediting the West, Diplomacy' 'URW: Discrediting Ukraine']",
         "['URW: Discrediting the West, Diplomacy: Other'\n 'URW: Discrediting Ukraine: Ukraine is associated with nazism']",
         "BG",
         "[13, 14]",
         "[84, 86]",
         "2",
         "2",
         "275",
         "251-500",
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0]"
        ],
        [
         "4",
         "A9_BG_3379.txt",
         "Британски дипломат обвини Запада за украинския конфликт — Responsible Statecraft\n\nБритански дипломат обвини Запада за украинския конфликт — Responsible Statecraft\nВ условията на трескаво лобиране от страна на Запада за изостряне на конфликта, много липсват гласовете на разума и на несъгласните с опосредстваната война на НАТО с Русия в Украйна, пише Responsible Statecraft. Но ги има. Сред тях е бившият икономически съветник на британското посолство в Москва Иън Прауд.\nВ книгата си \"Неудачникът в Москва: Как се провали британската дипломация в Русия?\" Прауд разказва, че след началото на украинската криза през 2014 г.за Лондон стана самоцел да накаже Русия с помощта на санкции и изолация. Прауд предупреждавал, че санкциите само ще вдъхновят Русия да се стреми към самодостатъчност. Обаче не го послушаха.\nТой беше особено впечатлен от това как Москва се справи с нестабилността, причинена от комбинация от западни санкции и намаляване на цените на енергията. А към момента на началото на СВО Русия вече можеше да сдържа опитите на западните страни да дестабилизират рублата и да сринат руската икономика.\nПрауд каза, че Лондон е допринесъл за провала на Минските споразумения, когато е убедил Европейския съвет да се съгласи, че санкциите срещу Русия няма да бъдат отменени до пълното изпълнение на споразуменията. Това даде на Киев допълнителен стимул да саботира тяхното спазване. Но Москва, за разлика от Киев, беше наистина заинтересована от изпълнението на споразуменията. Освен това Русия беше единствената страна по тези споразумения, която действаше добросъвестно.",
         "['URW: Discrediting the West, Diplomacy' 'URW: Praise of Russia']",
         "['URW: Discrediting the West, Diplomacy: Other'\n 'URW: Praise of Russia: Other']",
         "BG",
         "[19, 14]",
         "[86, 103]",
         "2",
         "2",
         "237",
         "101-250",
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n 0 0 0 0 0 0]"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>narratives</th>\n",
       "      <th>subnarratives</th>\n",
       "      <th>language</th>\n",
       "      <th>narrative_ids</th>\n",
       "      <th>subnarrative_ids</th>\n",
       "      <th>num_narratives</th>\n",
       "      <th>num_subnarratives</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_count_bin</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BG_670.txt</td>\n",
       "      <td>Опитът на колективния Запад да „обезкърви Руси...</td>\n",
       "      <td>[URW: Blaming the war on others rather than th...</td>\n",
       "      <td>[URW: Blaming the war on others rather than th...</td>\n",
       "      <td>BG</td>\n",
       "      <td>[11, 12, 14]</td>\n",
       "      <td>[88, 70, 74, 86]</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>248</td>\n",
       "      <td>101-250</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A7_URW_BG_4793.txt</td>\n",
       "      <td>Цончо Ганев, “Възраждане”: Обещали сме на Укра...</td>\n",
       "      <td>[URW: Discrediting Ukraine]</td>\n",
       "      <td>[URW: Discrediting Ukraine: Situation in Ukrai...</td>\n",
       "      <td>BG</td>\n",
       "      <td>[13]</td>\n",
       "      <td>[81]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>503</td>\n",
       "      <td>501-1000</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BG_3245.txt</td>\n",
       "      <td>Подкрепата за Киев от страна на Запада вече не...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy, URW: D...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy: The We...</td>\n",
       "      <td>BG</td>\n",
       "      <td>[13, 14]</td>\n",
       "      <td>[81, 90, 87]</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>190</td>\n",
       "      <td>101-250</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A9_BG_5190.txt</td>\n",
       "      <td>Дмитрий Медведев: НПО-та, спонсорирани от Соро...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy, URW: D...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy: Other,...</td>\n",
       "      <td>BG</td>\n",
       "      <td>[13, 14]</td>\n",
       "      <td>[84, 86]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>275</td>\n",
       "      <td>251-500</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A9_BG_3379.txt</td>\n",
       "      <td>Британски дипломат обвини Запада за украинския...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy, URW: P...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy: Other,...</td>\n",
       "      <td>BG</td>\n",
       "      <td>[19, 14]</td>\n",
       "      <td>[86, 103]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>237</td>\n",
       "      <td>101-250</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               text  \\\n",
       "0          BG_670.txt  Опитът на колективния Запад да „обезкърви Руси...   \n",
       "1  A7_URW_BG_4793.txt  Цончо Ганев, “Възраждане”: Обещали сме на Укра...   \n",
       "2         BG_3245.txt  Подкрепата за Киев от страна на Запада вече не...   \n",
       "3      A9_BG_5190.txt  Дмитрий Медведев: НПО-та, спонсорирани от Соро...   \n",
       "4      A9_BG_3379.txt  Британски дипломат обвини Запада за украинския...   \n",
       "\n",
       "                                          narratives  \\\n",
       "0  [URW: Blaming the war on others rather than th...   \n",
       "1                        [URW: Discrediting Ukraine]   \n",
       "2  [URW: Discrediting the West, Diplomacy, URW: D...   \n",
       "3  [URW: Discrediting the West, Diplomacy, URW: D...   \n",
       "4  [URW: Discrediting the West, Diplomacy, URW: P...   \n",
       "\n",
       "                                       subnarratives language narrative_ids  \\\n",
       "0  [URW: Blaming the war on others rather than th...       BG  [11, 12, 14]   \n",
       "1  [URW: Discrediting Ukraine: Situation in Ukrai...       BG          [13]   \n",
       "2  [URW: Discrediting the West, Diplomacy: The We...       BG      [13, 14]   \n",
       "3  [URW: Discrediting the West, Diplomacy: Other,...       BG      [13, 14]   \n",
       "4  [URW: Discrediting the West, Diplomacy: Other,...       BG      [19, 14]   \n",
       "\n",
       "   subnarrative_ids  num_narratives  num_subnarratives  word_count  \\\n",
       "0  [88, 70, 74, 86]               4                  4         248   \n",
       "1              [81]               1                  1         503   \n",
       "2      [81, 90, 87]               3                  3         190   \n",
       "3          [84, 86]               2                  2         275   \n",
       "4         [86, 103]               2                  2         237   \n",
       "\n",
       "  word_count_bin                                             labels  \n",
       "0        101-250  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, ...  \n",
       "1       501-1000  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "2        101-250  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...  \n",
       "3        251-500  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...  \n",
       "4        101-250  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data_management.loaders import load_labeled_df\n",
    "\n",
    "df = load_labeled_df('phase0_baseline_labeled.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5943059d",
   "metadata": {},
   "source": [
    "# Splitting the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a51aafd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (1699, 12)\n",
      "Train set shape: (1008, 12)\n",
      "Validation set shape: (324, 12)\n",
      "Test set shape: (367, 12)\n",
      "\n",
      "Example of train_df head:\n",
      "                    id                                               text  \\\n",
      "1   A7_URW_BG_4793.txt  Цончо Ганев, “Възраждане”: Обещали сме на Укра...   \n",
      "6           BG_855.txt  Русия забрани разпространението на десетки мед...   \n",
      "7           BG_751.txt  US военен: Путин ни изигра така, както Рейгън ...   \n",
      "15  A8_CC_BG_10189.txt  Д-р Симеонов: В обозримо бъдеще ни очакват екс...   \n",
      "20          BG_364.txt  Учените: На нова планета сме, ако градусите не...   \n",
      "\n",
      "                                           narratives  \\\n",
      "1                         [URW: Discrediting Ukraine]   \n",
      "6   [URW: Distrust towards Media, URW: Distrust to...   \n",
      "7   [URW: Praise of Russia, URW: Discrediting Ukra...   \n",
      "15  [CC: Amplifying Climate Fears, CC: Amplifying ...   \n",
      "20  [CC: Amplifying Climate Fears, CC: Amplifying ...   \n",
      "\n",
      "                                        subnarratives language narrative_ids  \\\n",
      "1   [URW: Discrediting Ukraine: Situation in Ukrai...       BG          [13]   \n",
      "6   [URW: Distrust towards Media: Other, URW: Dist...       BG      [20, 15]   \n",
      "7   [URW: Praise of Russia: Praise of Russian Pres...       BG  [19, 12, 13]   \n",
      "15  [CC: Amplifying Climate Fears: Amplifying exis...       BG           [0]   \n",
      "20  [CC: Amplifying Climate Fears: Other, CC: Ampl...       BG        [0, 9]   \n",
      "\n",
      "        subnarrative_ids  num_narratives  num_subnarratives  word_count  \\\n",
      "1                   [81]               1                  1         503   \n",
      "6          [92, 109, 94]               3                  3         339   \n",
      "7          [104, 74, 83]               3                  3         493   \n",
      "15              [22, 23]               2                  2         312   \n",
      "20  [65, 22, 24, 25, 26]               5                  5         410   \n",
      "\n",
      "   word_count_bin                                             labels  \n",
      "1        501-1000  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
      "6         251-500  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "7         251-500  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...  \n",
      "15        251-500  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "20        251-500  [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = df.index.to_numpy().reshape(-1, 1)\n",
    "y = np.array(df['labels'].tolist())\n",
    "\n",
    "train_val_indices, y_train_val, test_indices, y_test = iterative_train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "\n",
    "train_indices, y_train, val_indices, y_val = iterative_train_test_split(train_val_indices, y_train_val, test_size = 0.25)\n",
    "\n",
    "train_df = df.loc[train_indices.flatten()]\n",
    "val_df = df.loc[val_indices.flatten()]\n",
    "test_df = df.loc[test_indices.flatten()]\n",
    "\n",
    "\n",
    "# 5. Verify the results\n",
    "print(\"Original dataset shape:\", df.shape)\n",
    "print(\"Train set shape:\", train_df.shape)\n",
    "print(\"Validation set shape:\", val_df.shape)\n",
    "print(\"Test set shape:\", test_df.shape)\n",
    "\n",
    "print(\"\\nExample of train_df head:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4129a73a",
   "metadata": {},
   "source": [
    "# Tokenizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2416b2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/twoface/Documents/Passau/masterarbeit/hybrid-text-classification/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total labels: 117\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "MODEL_NAME = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3df03ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total labels: 117\n"
     ]
    }
   ],
   "source": [
    "num_total_labels = df['labels'].iloc[0].shape[0]\n",
    "print(f\"Number of total labels: {num_total_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a9b153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_management.label_parser import get_label_mappings\n",
    "\n",
    "label_to_id, id_to_label, narrative_to_subnarrative_ids = get_label_mappings()\n",
    "sub_to_narr_id_map = {}\n",
    "\n",
    "# Create a mapping from sub-narrative IDs to their parent narrative IDs\n",
    "for narr_id, sub_ids_list in narrative_to_subnarrative_ids.items():\n",
    "    for sub_id in sub_ids_list:\n",
    "        sub_to_narr_id_map[sub_id] = narr_id\n",
    "\n",
    "# This gives you a map like: { sub_id_A: narr_id_1, sub_id_B: narr_id_1, ... }\n",
    "# It's useful to also have a simple list of all parent-child ID pairs\n",
    "parent_child_pairs = list(sub_to_narr_id_map.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0b9cc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels = num_total_labels,\n",
    "    problem_type = 'multi_label_classification',\n",
    "    id2label = id_to_label,\n",
    "    label2id = label_to_id \n",
    ")\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5827788e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PyTorch datasets...\n",
      "PyTorch datasets created successfully.\n"
     ]
    }
   ],
   "source": [
    "from src.data_management.datasets import NarrativeClassificationDataset\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "print(\"Creating PyTorch datasets...\")\n",
    "train_dataset = NarrativeClassificationDataset(\n",
    "    train_df,\n",
    "    tokenizer,\n",
    "    max_length = MAX_LENGTH,\n",
    ")\n",
    "\n",
    "test_dataset = NarrativeClassificationDataset(\n",
    "    test_df,\n",
    "    tokenizer,\n",
    "    max_length = MAX_LENGTH,\n",
    ")\n",
    "\n",
    "val_dataset = NarrativeClassificationDataset(\n",
    "    val_df,\n",
    "    tokenizer,\n",
    "    max_length = MAX_LENGTH,\n",
    ")\n",
    "print(\"PyTorch datasets created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f107c2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataLoaders...\n",
      "DataLoaders created successfully.\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 117])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "print(\"Creating DataLoaders...\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 8,\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    num_workers = 8,\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    num_workers = 8,\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "print(\"DataLoaders created successfully.\")\n",
    "for batch in train_dataloader:\n",
    "    print(batch['input_ids'].shape) # Should be [BATCH_SIZE, MAX_TOKEN_LEN]\n",
    "    print(batch['labels'].shape)    # Should be [BATCH_SIZE, num_total_labels]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f703f9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=117, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move your model to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2be98987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer created successfully.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "print(\"Optimizer created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71579c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduler created successfully.\n"
     ]
    }
   ],
   "source": [
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "\n",
    "EPOCH = 10\n",
    "num_training_steps = len(train_dataloader) * EPOCH\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = num_training_steps\n",
    ")\n",
    "\n",
    "print(\"Scheduler created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5bb035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.engine import train_epoch, evaluate\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "H_LAMBDA = 1.5\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "\n",
    "# for epoch in tqdm(range(EPOCH), desc=\"Epochs\"):\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCH}\")\n",
    "#\n",
    "#     train_loss = train_epoch(\n",
    "#         model,\n",
    "#         train_dataloader,\n",
    "#         optimizer,\n",
    "#         scheduler,\n",
    "#         loss_function,\n",
    "#         device,\n",
    "#         parent_child_pairs,\n",
    "#         H_LAMBDA\n",
    "#     )\n",
    "    \n",
    "#     val_loss, metrics = evaluate(\n",
    "#         model,\n",
    "#         val_dataloader,\n",
    "#         loss_function,\n",
    "#         device,\n",
    "#         H_LAMBDA,\n",
    "#         parent_child_pairs,\n",
    "#         threshold=0.5 # Using a default threshold for validation during training\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Validation F1-score (micro): {metrics['f1_micro']:.4f}\")\n",
    "\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         patience_counter = 0\n",
    "#         # Save the best model\n",
    "#         torch.save(model.state_dict(), f'phase0_{MODEL_NAME}_best_model.bin')\n",
    "#         print(\"Best model saved.\")\n",
    "#     else:\n",
    "#         patience_counter += 1\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping triggered.\")\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4864fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading best model for threshold finding and final evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Raw Predictions: 100%|██████████| 21/21 [00:06<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for the best threshold to optimize f1_micro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching Thresholds: 100%|██████████| 81/81 [00:00<00:00, 186.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search complete!\n",
      "Best Threshold found: 0.86\n",
      "Best Validation F1_micro: 0.3980\n",
      "\n",
      "--- Final Evaluation on TEST set using the Optimal Threshold ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Raw Predictions: 100%|██████████| 24/24 [00:04<00:00,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Reportable Performance on Test Set:\n",
      "  - F1 Micro: 0.3382\n",
      "  - F1 Macro: 0.0753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- Import your new and existing functions ---\n",
    "# Your existing compute_metrics is inside engine.py\n",
    "from src.training.engine import get_raw_predictions, compute_metrics\n",
    "from src.utils.metrics import find_best_threshold\n",
    "\n",
    "# --- Assumed objects are available ---\n",
    "# model, val_dataloader, test_dataloader, device, parent_child_pairs\n",
    "MODEL_OUTPUT_PATH = \"models/phase0_xlmr_best_model.bin\"\n",
    "\n",
    "# 1. LOAD THE BEST MODEL WEIGHTS SAVED DURING TRAINING\n",
    "print(\"\\n--- Loading best model for threshold finding and final evaluation ---\")\n",
    "model.load_state_dict(torch.load(MODEL_OUTPUT_PATH))\n",
    "\n",
    "model.to(device) # Make sure model is on the correct device\n",
    "\n",
    "# 2. GET PREDICTIONS ON THE VALIDATION SET\n",
    "# Use the new, clean function from engine.py\n",
    "val_logits, val_true_labels = get_raw_predictions(model, val_dataloader, device)\n",
    "\n",
    "# 3. FIND THE OPTIMAL THRESHOLD\n",
    "# Use the new function from metrics.py\n",
    "optimal_threshold = find_best_threshold(\n",
    "    val_logits,\n",
    "    val_true_labels,\n",
    "    parent_child_pairs,\n",
    "    metric_to_optimize='f1_micro',\n",
    "    compute_metrics_fn=compute_metrics # Pass your metrics function\n",
    ")\n",
    "\n",
    "# 4. FINAL EVALUATION ON THE UNSEEN TEST SET\n",
    "print(\"\\n--- Final Evaluation on TEST set using the Optimal Threshold ---\")\n",
    "\n",
    "# Get raw predictions for the test set\n",
    "test_logits, test_true_labels = get_raw_predictions(model, test_dataloader, device)\n",
    "\n",
    "# Calculate final metrics using your original compute_metrics function\n",
    "# and the optimal_threshold you just found\n",
    "final_metrics = compute_metrics(\n",
    "    test_logits,\n",
    "    test_true_labels,\n",
    "    parent_child_pairs,\n",
    "    threshold=optimal_threshold\n",
    ")\n",
    "\n",
    "print(f\"Final Reportable Performance on Test Set:\")\n",
    "print(f\"  - F1 Micro: {final_metrics['f1_micro']:.4f}\")\n",
    "print(f\"  - F1 Macro: {final_metrics['f1_macro']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9449019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on the test set with the optimal threshold...\n",
      "Running evaluation on the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 24/24 [00:07<00:00,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.4266\n",
      "\n",
      "Test Set Metrics:\n",
      "f1_micro: 0.3382\n",
      "f1_macro: 0.0753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set with the optimal threshold\n",
    "print(\"Evaluating on the test set with the optimal threshold...\")\n",
    "# Load the best model before evaluating on the test set\n",
    "model.load_state_dict(torch.load(f'models/phase0_xlmr_best_model.bin'))\n",
    "test_loss, test_metrics = evaluate(\n",
    "    model,\n",
    "    test_dataloader,\n",
    "    loss_function,\n",
    "    device,\n",
    "    H_LAMBDA,\n",
    "    parent_child_pairs,\n",
    "    threshold=optimal_threshold\n",
    ")\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for key, value in test_metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae048224",
   "metadata": {},
   "source": [
    "# New data from the devset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532e4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_BEST_MODEL = 'models/phase0_xlmr_best_model.bin'\n",
    "MODEL_NAME = 'xlm-roberta-base'\n",
    "CONTINUAL_LEARNING_MODEL_PATH = 'models/phase0_xlmr_continual_learning_model.bin'\n",
    "CONTINUAL_LEARNING_EPOCHS = 5\n",
    "CONTINUAL_LEARNING_LR = 2e-6 \n",
    "CONTINUAL_LEARNING_PATIENCE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4668fc47",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_preparation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prepare_datasets\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Preparing original training data ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m (\n\u001b[32m      6\u001b[39m     original_train_dataset,\n\u001b[32m      7\u001b[39m     original_val_dataset,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     docs_folder=\u001b[33m'\u001b[39m\u001b[33mraw-documents\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from src.scripts.data_preparation import prepare_datasets\n",
    "\n",
    "print(\"--- Preparing original training data ---\")\n",
    "\n",
    "(\n",
    "    original_train_dataset,\n",
    "    original_val_dataset,\n",
    "    original_test_dataset,\n",
    "    tokenizer, \n",
    "    id_to_label, \n",
    "    label_to_id,\n",
    "    parent_child_pairs, \n",
    "    num_total_labels, \n",
    ") = prepare_datasets(\n",
    "    data_folder='data',\n",
    "    model_name=MODEL_NAME,\n",
    "    docs_folder='raw-documents'\n",
    ")\n",
    "\n",
    "print(\"\\nOriginal datasets created:\")\n",
    "print(f\"  - Original Train set size: {len(original_train_dataset)}\")\n",
    "print(f\"  - Original Validation set size: {len(original_val_dataset)}\")\n",
    "print(f\"  - Original Test set size: {len(original_test_dataset)}\")\n",
    "\n",
    "print(\"--- Preparing incremental training data from devset ---\")\n",
    "# We can reuse the same model name and max length from the initial setup.\n",
    "# The tokenizer is already loaded, but prepare_datasets will load it again.\n",
    "# This is okay for this demonstration.\n",
    "(\n",
    "    inc_train_dataset,\n",
    "    inc_val_dataset,\n",
    "    inc_test_dataset,\n",
    "    _, # tokenizer - assuming it's the same\n",
    "    _, # id_to_label - assuming it's the same\n",
    "    _,\n",
    "    _, # parent_child_pairs - assuming they are the same\n",
    "    _, # num_total_labels - assuming it's the same\n",
    ") = prepare_datasets(\n",
    "    data_folder='devset',\n",
    "    model_name=MODEL_NAME,\n",
    "    docs_folder='subtask-2-documents'\n",
    ")\n",
    "\n",
    "print(\"\\nIncremental datasets created:\")\n",
    "print(f\"  - Incremental Train set size: {len(inc_train_dataset)}\")\n",
    "print(f\"  - Incremental Validation set size: {len(inc_val_dataset)}\")\n",
    "print(f\"  - Incremental Test set size: {len(inc_test_dataset)}\")\n",
    "\n",
    "# For incremental training, you would typically use `inc_train_dataset`.\n",
    "# You might also combine it with the original training set or use `inc_val_dataset`\n",
    "# for evaluating the model's performance during continual learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23dc4153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Combined Training Dataset ---\n",
      "Original training set size: 995\n",
      "Incremental train set size: 105\n",
      "Incremental validation set size: 35\n",
      "Incremental test set size: 38\n",
      "Total combined training set size: 1173\n",
      "\n",
      "--- Evaluation Datasets ---\n",
      "Original validation set size: 337\n",
      "Original test set size: 367\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# Combine the original training data with all parts of the incremental data\n",
    "combined_train_dataset = ConcatDataset([\n",
    "    original_train_dataset,\n",
    "    inc_train_dataset,\n",
    "    inc_val_dataset,\n",
    "    inc_test_dataset\n",
    "])\n",
    "\n",
    "print(f\"--- Combined Training Dataset ---\")\n",
    "print(f\"Original training set size: {len(original_train_dataset)}\")\n",
    "print(f\"Incremental train set size: {len(inc_train_dataset)}\")\n",
    "print(f\"Incremental validation set size: {len(inc_val_dataset)}\")\n",
    "print(f\"Incremental test set size: {len(inc_test_dataset)}\")\n",
    "print(f\"Total combined training set size: {len(combined_train_dataset)}\")\n",
    "\n",
    "# The original validation and test sets remain unchanged for final evaluation\n",
    "print(f\"\\n--- Evaluation Datasets ---\")\n",
    "print(f\"Original validation set size: {len(original_val_dataset)}\")\n",
    "print(f\"Original test set size: {len(original_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bd3b4c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_to_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSequenceClassification\n\u001b[32m      5\u001b[39m best_val_loss = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m model = AutoModelForSequenceClassification.from_pretrained(\n\u001b[32m      8\u001b[39m     MODEL_NAME,\n\u001b[32m      9\u001b[39m     num_labels=num_total_labels,\n\u001b[32m     10\u001b[39m     problem_type=\u001b[33m'\u001b[39m\u001b[33mmulti_label_classification\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     11\u001b[39m     id2label=id_to_label,\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     label2id=\u001b[43mlabel_to_id\u001b[49m\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m model.load_state_dict(torch.load(PATH_TO_BEST_MODEL))\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(CONTINUAL_LEARNING_EPOCHS), desc=\u001b[33m\"\u001b[39m\u001b[33mContinual Learning Epochs\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mNameError\u001b[39m: name 'label_to_id' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from src.training.engine import train_epoch, evaluate\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_total_labels,\n",
    "    problem_type='multi_label_classification',\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "model.load_state_dict(torch.load(PATH_TO_BEST_MODEL))\n",
    "\n",
    "for epoch in tqdm(range(CONTINUAL_LEARNING_EPOCHS), desc=\"Continual Learning Epochs\"):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{CONTINUAL_LEARNING_EPOCHS}\")\n",
    "    \n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        DataLoader(combined_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True),\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss_function,\n",
    "        device,\n",
    "        parent_child_pairs,\n",
    "        H_LAMBDA\n",
    "    )\n",
    "    \n",
    "    val_loss, metrics = evaluate(\n",
    "        model,\n",
    "        DataLoader(original_val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=True),\n",
    "        loss_function,\n",
    "        device,\n",
    "        H_LAMBDA,\n",
    "        parent_child_pairs,\n",
    "        threshold=optimal_threshold\n",
    "    )\n",
    "    \n",
    "    print(f\"Validation F1-score (micro): {metrics['f1_micro']:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), CONTINUAL_LEARNING_MODEL_PATH)\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= CONTINUAL_LEARNING_PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
