{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44df6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "# Set the current working directory to the project root\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_management.loaders import load_labeled_df\n",
    "\n",
    "df = load_labeled_df('phase0_baseline_labeled.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5943059d",
   "metadata": {},
   "source": [
    "# Splitting the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51aafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = df.index.to_numpy().reshape(-1, 1)\n",
    "y = np.array(df['labels'].tolist())\n",
    "\n",
    "train_val_indices, y_train_val, test_indices, y_test = iterative_train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "\n",
    "train_indices, y_train, val_indices, y_val = iterative_train_test_split(train_val_indices, y_train_val, test_size = 0.25)\n",
    "\n",
    "train_df = df.loc[train_indices.flatten()]\n",
    "val_df = df.loc[val_indices.flatten()]\n",
    "test_df = df.loc[test_indices.flatten()]\n",
    "\n",
    "\n",
    "# 5. Verify the results\n",
    "print(\"Original dataset shape:\", df.shape)\n",
    "print(\"Train set shape:\", train_df.shape)\n",
    "print(\"Validation set shape:\", val_df.shape)\n",
    "print(\"Test set shape:\", test_df.shape)\n",
    "\n",
    "print(\"\\nExample of train_df head:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4129a73a",
   "metadata": {},
   "source": [
    "# Tokenizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2416b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "MODEL_NAME = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df03ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total_labels = df['labels'].iloc[0].shape[0]\n",
    "print(f\"Number of total labels: {num_total_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9b153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_management.label_parser import get_label_mappings\n",
    "\n",
    "label_to_id, id_to_label, narrative_to_subnarrative_ids = get_label_mappings()\n",
    "sub_to_narr_id_map = {}\n",
    "\n",
    "# Create a mapping from sub-narrative IDs to their parent narrative IDs\n",
    "for narr_id, sub_ids_list in narrative_to_subnarrative_ids.items():\n",
    "    for sub_id in sub_ids_list:\n",
    "        sub_to_narr_id_map[sub_id] = narr_id\n",
    "\n",
    "# This gives you a map like: { sub_id_A: narr_id_1, sub_id_B: narr_id_1, ... }\n",
    "# It's useful to also have a simple list of all parent-child ID pairs\n",
    "parent_child_pairs = list(sub_to_narr_id_map.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b9cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels = num_total_labels,\n",
    "    problem_type = 'multi_label_classification',\n",
    "    id2label = id_to_label,\n",
    "    label2id = label_to_id \n",
    ")\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_management.datasets import NarrativeClassificationDataset\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "print(\"Creating PyTorch datasets...\")\n",
    "train_dataset = NarrativeClassificationDataset(\n",
    "    train_df,\n",
    "    tokenizer,\n",
    "    max_length = MAX_LENGTH,\n",
    ")\n",
    "\n",
    "test_dataset = NarrativeClassificationDataset(\n",
    "    test_df,\n",
    "    tokenizer,\n",
    "    max_length = MAX_LENGTH,\n",
    ")\n",
    "\n",
    "val_dataset = NarrativeClassificationDataset(\n",
    "    val_df,\n",
    "    tokenizer,\n",
    "    max_length = MAX_LENGTH,\n",
    ")\n",
    "print(\"PyTorch datasets created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f107c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "print(\"Creating DataLoaders...\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 8,\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    num_workers = 8,\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    num_workers = 8,\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "print(\"DataLoaders created successfully.\")\n",
    "for batch in train_dataloader:\n",
    "    print(batch['input_ids'].shape) # Should be [BATCH_SIZE, MAX_TOKEN_LEN]\n",
    "    print(batch['labels'].shape)    # Should be [BATCH_SIZE, num_total_labels]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f703f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move your model to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be98987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "print(\"Optimizer created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71579c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "\n",
    "EPOCH = 10\n",
    "num_training_steps = len(train_dataloader) * EPOCH\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = num_training_steps\n",
    ")\n",
    "\n",
    "print(\"Scheduler created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5bb035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.engine import train_epoch, evaluate\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "H_LAMBDA = 1.5\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "\n",
    "# for epoch in tqdm(range(EPOCH), desc=\"Epochs\"):\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCH}\")\n",
    "#\n",
    "#     train_loss = train_epoch(\n",
    "#         model,\n",
    "#         train_dataloader,\n",
    "#         optimizer,\n",
    "#         scheduler,\n",
    "#         loss_function,\n",
    "#         device,\n",
    "#         parent_child_pairs,\n",
    "#         H_LAMBDA\n",
    "#     )\n",
    "    \n",
    "#     val_loss, metrics = evaluate(\n",
    "#         model,\n",
    "#         val_dataloader,\n",
    "#         loss_function,\n",
    "#         device,\n",
    "#         H_LAMBDA,\n",
    "#         parent_child_pairs,\n",
    "#         threshold=0.5 # Using a default threshold for validation during training\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Validation F1-score (micro): {metrics['f1_micro']:.4f}\")\n",
    "\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         patience_counter = 0\n",
    "#         # Save the best model\n",
    "#         torch.save(model.state_dict(), f'phase0_{MODEL_NAME}_best_model.bin')\n",
    "#         print(\"Best model saved.\")\n",
    "#     else:\n",
    "#         patience_counter += 1\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping triggered.\")\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4864fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- Import your new and existing functions ---\n",
    "# Your existing compute_metrics is inside engine.py\n",
    "from src.training.engine import get_raw_predictions, compute_metrics\n",
    "from src.utils.metrics import find_best_threshold\n",
    "\n",
    "# --- Assumed objects are available ---\n",
    "# model, val_dataloader, test_dataloader, device, parent_child_pairs\n",
    "MODEL_OUTPUT_PATH = \"models/phase0_xlmr_best_model.bin\"\n",
    "\n",
    "# 1. LOAD THE BEST MODEL WEIGHTS SAVED DURING TRAINING\n",
    "print(\"\\n--- Loading best model for threshold finding and final evaluation ---\")\n",
    "model.load_state_dict(torch.load(MODEL_OUTPUT_PATH))\n",
    "\n",
    "model.to(device) # Make sure model is on the correct device\n",
    "\n",
    "# 2. GET PREDICTIONS ON THE VALIDATION SET\n",
    "# Use the new, clean function from engine.py\n",
    "val_logits, val_true_labels = get_raw_predictions(model, val_dataloader, device)\n",
    "\n",
    "# 3. FIND THE OPTIMAL THRESHOLD\n",
    "# Use the new function from metrics.py\n",
    "optimal_threshold = find_best_threshold(\n",
    "    val_logits,\n",
    "    val_true_labels,\n",
    "    parent_child_pairs,\n",
    "    metric_to_optimize='f1_micro',\n",
    "    compute_metrics_fn=compute_metrics # Pass your metrics function\n",
    ")\n",
    "\n",
    "# 4. FINAL EVALUATION ON THE UNSEEN TEST SET\n",
    "print(\"\\n--- Final Evaluation on TEST set using the Optimal Threshold ---\")\n",
    "\n",
    "# Get raw predictions for the test set\n",
    "test_logits, test_true_labels = get_raw_predictions(model, test_dataloader, device)\n",
    "\n",
    "# Calculate final metrics using your original compute_metrics function\n",
    "# and the optimal_threshold you just found\n",
    "final_metrics = compute_metrics(\n",
    "    test_logits,\n",
    "    test_true_labels,\n",
    "    parent_child_pairs,\n",
    "    threshold=optimal_threshold\n",
    ")\n",
    "\n",
    "print(f\"Final Reportable Performance on Test Set:\")\n",
    "print(f\"  - F1 Micro: {final_metrics['f1_micro']:.4f}\")\n",
    "print(f\"  - F1 Macro: {final_metrics['f1_macro']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set with the optimal threshold\n",
    "print(\"Evaluating on the test set with the optimal threshold...\")\n",
    "# Load the best model before evaluating on the test set\n",
    "model.load_state_dict(torch.load(f'models/phase0_xlmr_best_model.bin'))\n",
    "test_loss, test_metrics = evaluate(\n",
    "    model,\n",
    "    test_dataloader,\n",
    "    loss_function,\n",
    "    device,\n",
    "    H_LAMBDA,\n",
    "    parent_child_pairs,\n",
    "    threshold=optimal_threshold\n",
    ")\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for key, value in test_metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae048224",
   "metadata": {},
   "source": [
    "# New data from the devset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_BEST_MODEL = 'models/phase0_xlmr_best_model.bin'\n",
    "MODEL_NAME = 'xlm-roberta-base'\n",
    "CONTINUAL_LEARNING_MODEL_PATH = 'models/phase0_xlmr_continual_learning_model.bin'\n",
    "CONTINUAL_LEARNING_EPOCHS = 5\n",
    "CONTINUAL_LEARNING_LR = 2e-6 \n",
    "CONTINUAL_LEARNING_PATIENCE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4668fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.data_preparation import prepare_datasets\n",
    "\n",
    "print(\"--- Preparing original training data ---\")\n",
    "\n",
    "(\n",
    "    original_train_dataset,\n",
    "    original_val_dataset,\n",
    "    original_test_dataset,\n",
    "    tokenizer, \n",
    "    id_to_label, \n",
    "    label_to_id,\n",
    "    parent_child_pairs, \n",
    "    num_total_labels, \n",
    ") = prepare_datasets(\n",
    "    data_folder='data',\n",
    "    model_name=MODEL_NAME,\n",
    "    docs_folder='raw-documents'\n",
    ")\n",
    "\n",
    "print(\"\\nOriginal datasets created:\")\n",
    "print(f\"  - Original Train set size: {len(original_train_dataset)}\")\n",
    "print(f\"  - Original Validation set size: {len(original_val_dataset)}\")\n",
    "print(f\"  - Original Test set size: {len(original_test_dataset)}\")\n",
    "\n",
    "print(\"--- Preparing incremental training data from devset ---\")\n",
    "# We can reuse the same model name and max length from the initial setup.\n",
    "# The tokenizer is already loaded, but prepare_datasets will load it again.\n",
    "# This is okay for this demonstration.\n",
    "(\n",
    "    inc_train_dataset,\n",
    "    inc_val_dataset,\n",
    "    inc_test_dataset,\n",
    "    _, # tokenizer - assuming it's the same\n",
    "    _, # id_to_label - assuming it's the same\n",
    "    _,\n",
    "    _, # parent_child_pairs - assuming they are the same\n",
    "    _, # num_total_labels - assuming it's the same\n",
    ") = prepare_datasets(\n",
    "    data_folder='devset',\n",
    "    model_name=MODEL_NAME,\n",
    "    docs_folder='subtask-2-documents'\n",
    ")\n",
    "\n",
    "print(\"\\nIncremental datasets created:\")\n",
    "print(f\"  - Incremental Train set size: {len(inc_train_dataset)}\")\n",
    "print(f\"  - Incremental Validation set size: {len(inc_val_dataset)}\")\n",
    "print(f\"  - Incremental Test set size: {len(inc_test_dataset)}\")\n",
    "\n",
    "# For incremental training, you would typically use `inc_train_dataset`.\n",
    "# You might also combine it with the original training set or use `inc_val_dataset`\n",
    "# for evaluating the model's performance during continual learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc4153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# Combine the original training data with all parts of the incremental data\n",
    "combined_train_dataset = ConcatDataset([\n",
    "    original_train_dataset,\n",
    "    inc_train_dataset,\n",
    "    inc_val_dataset,\n",
    "    inc_test_dataset\n",
    "])\n",
    "\n",
    "print(f\"--- Combined Training Dataset ---\")\n",
    "print(f\"Original training set size: {len(original_train_dataset)}\")\n",
    "print(f\"Incremental train set size: {len(inc_train_dataset)}\")\n",
    "print(f\"Incremental validation set size: {len(inc_val_dataset)}\")\n",
    "print(f\"Incremental test set size: {len(inc_test_dataset)}\")\n",
    "print(f\"Total combined training set size: {len(combined_train_dataset)}\")\n",
    "\n",
    "# The original validation and test sets remain unchanged for final evaluation\n",
    "print(f\"\\n--- Evaluation Datasets ---\")\n",
    "print(f\"Original validation set size: {len(original_val_dataset)}\")\n",
    "print(f\"Original test set size: {len(original_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from src.training.engine import train_epoch, evaluate\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_total_labels,\n",
    "    problem_type='multi_label_classification',\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "model.load_state_dict(torch.load(PATH_TO_BEST_MODEL))\n",
    "\n",
    "for epoch in tqdm(range(CONTINUAL_LEARNING_EPOCHS), desc=\"Continual Learning Epochs\"):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{CONTINUAL_LEARNING_EPOCHS}\")\n",
    "    \n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        DataLoader(combined_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True),\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss_function,\n",
    "        device,\n",
    "        parent_child_pairs,\n",
    "        H_LAMBDA\n",
    "    )\n",
    "    \n",
    "    val_loss, metrics = evaluate(\n",
    "        model,\n",
    "        DataLoader(original_val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=True),\n",
    "        loss_function,\n",
    "        device,\n",
    "        H_LAMBDA,\n",
    "        parent_child_pairs,\n",
    "        threshold=optimal_threshold\n",
    "    )\n",
    "    \n",
    "    print(f\"Validation F1-score (micro): {metrics['f1_micro']:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), CONTINUAL_LEARNING_MODEL_PATH)\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= CONTINUAL_LEARNING_PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
