{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea762ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "# Set the current working directory to the project root\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f592dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.data_preparation import prepare_dataframes\n",
    "\n",
    "DATA_FOLDER = 'data'\n",
    "DOCS_FOLDER = 'raw-documents'\n",
    "\n",
    "# This function loads the data, splits it into train, val, and test sets, and returns them as pandas DataFrames\n",
    "train_df, val_df, test_df, id_to_label, label_to_id, parent_child_pairs = prepare_dataframes(\n",
    "    data_folder=DATA_FOLDER,\n",
    "    docs_folder=DOCS_FOLDER\n",
    ")\n",
    "\n",
    "print(f\"Train df: {train_df.shape}\")\n",
    "print(f\"Val df: {val_df.shape}\")\n",
    "print(f\"Test df: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c03a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from src.data_management.preprocessor import binarize_labels\n",
    "from src.data_management.processing import process_dataframe_for_training\n",
    "\n",
    "# Load the generated subnarrative texts\n",
    "with open(\"generated_subnarrative_texts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    generated_subnarratives = json.load(f)\n",
    "\n",
    "# Create a DataFrame from the generated data\n",
    "rows = []\n",
    "for item in generated_subnarratives:\n",
    "    for text in item['generated_texts']:\n",
    "        rows.append({\n",
    "            'text': text,\n",
    "            'subnarratives': [item['subnarrative']]\n",
    "        })\n",
    "new_data_df = pd.DataFrame(rows)\n",
    "\n",
    "new_data_df['narratives'] = new_data_df['subnarratives'].apply(lambda x: [subn.split(':')[0] + ':' + subn.split(':')[1] for subn in x])\n",
    "\n",
    "# Assuming label_to_id is defined somewhere in the code\n",
    "# Get all possible ids from the label_to_id mapping\n",
    "all_ids = list(id_to_label.keys())\n",
    "\n",
    "# Process the new data to create binarized labels\n",
    "new_data_df = process_dataframe_for_training(new_data_df, label_to_id, all_ids)\n",
    "# Add article id and language columns\n",
    "new_data_df = new_data_df.reset_index(drop=True)\n",
    "new_data_df['id'] = new_data_df.index.map(lambda i: f\"GEN_EN_{i:05d}.txt\")\n",
    "new_data_df['language'] = \"EN\"\n",
    "\n",
    "# Concatenate the new generated data with the existing train_df\n",
    "augmented_train_df = pd.concat([train_df, new_data_df], ignore_index=True)\n",
    "print(f\"Augmented train df: {augmented_train_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e319f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.setup import load_model_and_tokenizer\n",
    "import torch\n",
    "\n",
    "# --- Initialize Tokenizer and Load Best Model ---\n",
    "MODEL_NAME = 'xlm-roberta-base'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "num_total_labels = len(id_to_label)\n",
    "\n",
    "# Load the best model and tokenizer using the helper function\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "    model_name=MODEL_NAME,\n",
    "    device=device,\n",
    "    num_total_labels=num_total_labels,\n",
    "    id_to_label=id_to_label,\n",
    "    label_to_id=label_to_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e0ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_management.datasets import NarrativeClassificationDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- Create PyTorch Datasets ---\n",
    "MODEL_NAME = 'xlm-roberta-base'\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "\n",
    "augmented_train_dataset = NarrativeClassificationDataset(\n",
    "    augmented_train_df,\n",
    "    tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "val_dataset = NarrativeClassificationDataset(\n",
    "    val_df,\n",
    "    tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "# --- Create PyTorch DataLoaders ---\n",
    "train_dataloader = DataLoader(\n",
    "    augmented_train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Train dataloader size: {len(train_dataloader)}\")\n",
    "print(f\"Validation dataloader size: {len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2640e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute pos_weight for BCEWithLogitsLoss ---\n",
    "# pos_weight = (num_negative / num_positive) for each class\n",
    "labels = np.stack(augmented_train_df['labels'].values)\n",
    "num_pos = labels.sum(axis=0)\n",
    "num_neg = labels.shape[0] - num_pos\n",
    "# Avoid division by zero\n",
    "pos_weight = torch.tensor(num_neg / (num_pos + 1e-8), dtype=torch.float32, device=device)\n",
    "print(f\"pos_weight shape: {pos_weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb80e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from src.training.engine import train_epoch, evaluate\n",
    "from src.training.setup import setup_optimizer_and_scheduler\n",
    "\n",
    "# --- Training Setup ---\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 2e-5\n",
    "MODEL_SAVE_PATH = 'models/phase1_xlmr_augmented_best_model.bin'\n",
    "PATIENCE = 3\n",
    "H_LAMBDA = 1.5 # Hierarchical loss penalty\n",
    "\n",
    "optimizer, scheduler = setup_optimizer_and_scheduler(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    EPOCHS,\n",
    "    LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Use pos_weight in the loss function\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
    "    print(f\"--- Epoch {epoch+1}/{EPOCHS} ---\")\n",
    "\n",
    "    # --- Training Phase ---\n",
    "    train_loss = train_epoch(\n",
    "        model=model,\n",
    "        train_dataloader= train_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        loss_function=loss_function,\n",
    "        device=device,\n",
    "        H_LAMBDA=H_LAMBDA,\n",
    "        parent_child_pairs=parent_child_pairs,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(f\"Average Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    val_loss, metrics = evaluate(\n",
    "        model=model,\n",
    "        eval_dataloader=val_dataloader,\n",
    "        loss_function=loss_function,\n",
    "        device=device,\n",
    "        H_LAMBDA=H_LAMBDA,\n",
    "        parent_child_pairs=parent_child_pairs,\n",
    "        threshold=0.5\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"Average Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation F1-score (micro): {metrics['f1_micro']:.4f}\")\n",
    "\n",
    "\n",
    "    # --- Early Stopping ---\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(f\"Validation loss improved. Saved new best model to {MODEL_SAVE_PATH}\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"Validation loss did not improve for {epochs_no_improve} epoch(s).\")\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "        \n",
    "print(\"Training complete. Best model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447e873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the best threshold for the validation set\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from src.training.engine import get_raw_predictions, compute_metrics\n",
    "from src.utils.metrics import find_per_level_thresholds\n",
    "from src.data_management.datasets import NarrativeClassificationDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# --- Load Best Model ---\n",
    "print(\"\\n--- Loading best model for threshold finding ---\")\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "model.to(device)\n",
    "\n",
    "# --- Get Predictions on Validation Set ---\n",
    "print(\"Getting raw predictions from the validation set...\")\n",
    "val_logits, val_true_labels = get_raw_predictions(model, val_dataloader, device)\n",
    "print(\"Raw predictions obtained.\")\n",
    "\n",
    "# --- Find Optimal Per-Level Thresholds ---\n",
    "print(\"\\n--- Finding the optimal per-level thresholds ---\")\n",
    "val_probs = 1 / (1 + np.exp(-val_logits))\n",
    "\n",
    "# Identify narrative and subnarrative indices\n",
    "narrative_indices = [i for i, label in id_to_label.items() if label.count(\":\") == 1]\n",
    "subnarrative_indices = [i for i, label in id_to_label.items() if label.count(\":\") == 2]\n",
    "\n",
    "thresholds_result = find_per_level_thresholds(\n",
    "    probabilities=val_probs,\n",
    "    true_labels=val_true_labels,\n",
    "    narrative_indices=narrative_indices,\n",
    "    subnarrative_indices=subnarrative_indices,\n",
    "    parent_child_pairs=parent_child_pairs\n",
    ")\n",
    "\n",
    "narrative_threshold = thresholds_result[\"narrative_threshold\"]\n",
    "subnarrative_threshold = thresholds_result[\"subnarrative_threshold\"]\n",
    "print(f\"\\nOptimal narrative threshold: {narrative_threshold:.4f}\")\n",
    "print(f\"Optimal subnarrative threshold: {subnarrative_threshold:.4f}\")\n",
    "\n",
    "# --- Final Evaluation on the UNSEEN TEST SET ---\n",
    "print(\"\\n--- Final Evaluation on TEST set using the Optimal Per-Level Thresholds ---\")\n",
    "\n",
    "test_dataset = NarrativeClassificationDataset(\n",
    "    test_df,\n",
    "    tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Get raw predictions for the test set\n",
    "test_logits, test_true_labels = get_raw_predictions(model, test_dataloader, device)\n",
    "test_probs = 1 / (1 + np.exp(-test_logits))\n",
    "\n",
    "# Apply per-level thresholds to get binary predictions\n",
    "binary_preds = np.zeros_like(test_probs, dtype=int)\n",
    "binary_preds[:, narrative_indices] = (test_probs[:, narrative_indices] > narrative_threshold).astype(int)\n",
    "binary_preds[:, subnarrative_indices] = (test_probs[:, subnarrative_indices] > subnarrative_threshold).astype(int)\n",
    "\n",
    "# Optionally, apply hierarchical correction if needed\n",
    "if parent_child_pairs:\n",
    "    for sub_id, narr_id in parent_child_pairs:\n",
    "        inconsistent_mask = (binary_preds[:, sub_id] == 1) & (binary_preds[:, narr_id] == 0)\n",
    "        binary_preds[inconsistent_mask, sub_id] = 0\n",
    "\n",
    "# Calculate final metrics using the per-level thresholds\n",
    "from sklearn.metrics import f1_score\n",
    "f1_micro = f1_score(test_true_labels, binary_preds, average='micro', zero_division=0)\n",
    "f1_macro = f1_score(test_true_labels, binary_preds, average='macro', zero_division=0)\n",
    "\n",
    "print(f\"\\nFinal Reportable Performance on Test Set:\")\n",
    "print(f\"  - F1 Micro: {f1_micro:.4f}\")\n",
    "print(f\"  - F1 Macro: {f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea15ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'models/phase1_xlmr_augmented_best_model.bin'\n",
    "TOKENIZER_NAME = 'xlm-roberta-base'\n",
    "TEST_ARTICLES_PATH = 'testset/EN/subtask-2-documents/'\n",
    "OUTPUT_FILE = 'testset/en_predictions_augmented.txt'\n",
    "OPTIMAL_THRESHOLD = 0.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398fa6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_articles(folder_path):\n",
    "    \"\"\"Loads all .txt files from a folder.\"\"\"\n",
    "    articles = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as f:\n",
    "                articles.append({'article_id': filename, 'text': f.read()})\n",
    "    return pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657e0c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference.narrative_predictor import NarrativePredictor\n",
    "\n",
    "# Prepare label maps for the predictor\n",
    "label_maps = {\n",
    "    \"id2label\": id_to_label,\n",
    "    \"label2id\": label_to_id,\n",
    "    \"parent_child_pairs\": parent_child_pairs\n",
    "}\n",
    "\n",
    "TOKENIZER_NAME = 'xlm-roberta-base'\n",
    "\n",
    "# Initialize the predictor with the new model\n",
    "predictor = NarrativePredictor(MODEL_SAVE_PATH, TOKENIZER_NAME, label_maps)\n",
    "\n",
    "# Set optimal thresholds if available\n",
    "predictor.set_thresholds(0.71, 0.69)\n",
    "\n",
    "print(f\"Loading articles from {TEST_ARTICLES_PATH}...\")\n",
    "df_test = load_articles(TEST_ARTICLES_PATH)\n",
    "texts_to_predict = df_test['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c635f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.prediction_output import write_predictions_to_txt\n",
    "\n",
    "# Use the helper function to run predictions and write to .txt file\n",
    "write_predictions_to_txt(predictor, df_test, OUTPUT_FILE)\n",
    "\n",
    "print(f\"Predictions written to {OUTPUT_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
