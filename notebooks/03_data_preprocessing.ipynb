{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf6c058",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63c02e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "# Set the current working directory to the project root\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f153cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_management.label_parser import parse_json_for_narratives_subnarratives, create_label_mappings\n",
    "\n",
    "taxonomy_path = 'data/taxonomy.json'\n",
    "df = pd.read_parquet('data/processed/phase0_baseline.parquet')\n",
    "\n",
    "all_narratives, all_subnarratives = parse_json_for_narratives_subnarratives(taxonomy_path)\n",
    "label_to_id, id_to_label, narrative_to_subnarrative_ids = create_label_mappings(all_narratives, all_subnarratives)\n",
    "all_ids = list(id_to_label.keys())\n",
    "\n",
    "# convert the numpy arrays to lists\n",
    "df['narratives'] = df['narratives'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n",
    "df['subnarratives'] = df['subnarratives'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n",
    "df['narrative_ids'] = df['narrative_ids'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n",
    "df['subnarrative_ids'] = df['subnarrative_ids'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d169e",
   "metadata": {},
   "source": [
    "# Adding the bit vector labels to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "946526c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   id                                               text  \\\n",
      "0          BG_670.txt  Опитът на колективния Запад да „обезкърви Руси...   \n",
      "1  A7_URW_BG_4793.txt  Цончо Ганев, “Възраждане”: Обещали сме на Укра...   \n",
      "2         BG_3245.txt  Подкрепата за Киев от страна на Запада вече не...   \n",
      "3      A9_BG_5190.txt  Дмитрий Медведев: НПО-та, спонсорирани от Соро...   \n",
      "4      A9_BG_3379.txt  Британски дипломат обвини Запада за украинския...   \n",
      "\n",
      "                                          narratives  \\\n",
      "0  [URW: Blaming the war on others rather than th...   \n",
      "1                        [URW: Discrediting Ukraine]   \n",
      "2  [URW: Discrediting the West, Diplomacy, URW: D...   \n",
      "3  [URW: Discrediting the West, Diplomacy, URW: D...   \n",
      "4  [URW: Discrediting the West, Diplomacy, URW: P...   \n",
      "\n",
      "                                       subnarratives language narrative_ids  \\\n",
      "0  [URW: Blaming the war on others rather than th...       BG  [11, 12, 14]   \n",
      "1  [URW: Discrediting Ukraine: Situation in Ukrai...       BG          [13]   \n",
      "2  [URW: Discrediting the West, Diplomacy: The We...       BG      [13, 14]   \n",
      "3  [URW: Discrediting the West, Diplomacy: Other,...       BG      [13, 14]   \n",
      "4  [URW: Discrediting the West, Diplomacy: Other,...       BG      [19, 14]   \n",
      "\n",
      "   subnarrative_ids  num_narratives  num_subnarratives  word_count  \\\n",
      "0  [88, 70, 74, 86]               4                  4         248   \n",
      "1              [81]               1                  1         503   \n",
      "2      [81, 90, 87]               3                  3         190   \n",
      "3          [84, 86]               2                  2         275   \n",
      "4         [86, 103]               2                  2         237   \n",
      "\n",
      "  word_count_bin                                             labels  \n",
      "0        101-250  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, ...  \n",
      "1       501-1000  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
      "2        101-250  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...  \n",
      "3        251-500  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...  \n",
      "4        101-250  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n"
     ]
    }
   ],
   "source": [
    "from src.data_management.preprocessor import binarize_labels\n",
    "\n",
    "# concat the narratives and subnarratives\n",
    "df['labels'] = df.apply(lambda row: row['narrative_ids'] + row['subnarrative_ids'], axis=1)\n",
    "# binarize the labels\n",
    "df['labels'] = df['labels'].apply(lambda x: binarize_labels(x, all_ids))\n",
    "# show the first 5 rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e8d2b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame exported to: data/processed/phase0_baseline_labeled.parquet\n"
     ]
    }
   ],
   "source": [
    "output_parquet_path = 'data/processed/phase0_baseline_labeled.parquet'\n",
    "df.to_parquet(output_parquet_path, index=False)\n",
    "print(f\"DataFrame exported to: {output_parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca7948d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/twoface/Documents/Passau/masterarbeit/hybrid-text-classification/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c9b0db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_text'] = df['text'].apply(lambda x: tokenizer(x, truncation=True, padding='max_length', max_length=1024, return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aca5601",
   "metadata": {},
   "source": [
    "# Saving Tokenized Data in a Reloadable Format\n",
    "\n",
    "The `tokenized_text` column currently contains PyTorch tensors. To save this to Parquet in a way that's easy to reload, we'll extract the `input_ids` and `attention_mask` and convert them to lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2035593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with tokenized data (as lists) saved to: data/processed/phase0_baseline_tokenized_and_labeled.parquet\n",
      "Preview of the saved DataFrame structure:\n",
      "                                                text  \\\n",
      "0  Опитът на колективния Запад да „обезкърви Руси...   \n",
      "1  Цончо Ганев, “Възраждане”: Обещали сме на Укра...   \n",
      "2  Подкрепата за Киев от страна на Запада вече не...   \n",
      "3  Дмитрий Медведев: НПО-та, спонсорирани от Соро...   \n",
      "4  Британски дипломат обвини Запада за украинския...   \n",
      "\n",
      "                                              labels  \\\n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, ...   \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
      "\n",
      "                                      input_ids_list  \\\n",
      "0  [0, 1089, 22617, 1669, 29, 47829, 2097, 32275,...   \n",
      "1  [0, 160480, 108723, 45653, 3407, 4, 52, 2354, ...   \n",
      "2  [0, 10405, 24724, 2374, 205, 61, 94511, 183, 8...   \n",
      "3  [0, 154888, 189322, 12, 3580, 49551, 9, 205, 4...   \n",
      "4  [0, 57172, 1707, 36029, 206778, 180774, 61, 17...   \n",
      "\n",
      "                                 attention_mask_list  \n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n"
     ]
    }
   ],
   "source": [
    "df['input_ids_list'] = df['tokenized_text'].apply(lambda x: x['input_ids'].squeeze().tolist())\n",
    "df['attention_mask_list'] = df['tokenized_text'].apply(lambda x: x['attention_mask'].squeeze().tolist())\n",
    "\n",
    "df_to_save = df[['text', 'labels', 'input_ids_list', 'attention_mask_list']].copy()\n",
    "\n",
    "output_parquet_path_tokenized = 'data/processed/phase0_baseline_tokenized_and_labeled.parquet'\n",
    "df_to_save.to_parquet(output_parquet_path_tokenized, index=False)\n",
    "\n",
    "print(f\"DataFrame with tokenized data (as lists) saved to: {output_parquet_path_tokenized}\")\n",
    "print(\"Preview of the saved DataFrame structure:\")\n",
    "print(df_to_save.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feac3ed5",
   "metadata": {},
   "source": [
    "# Reloading Tokenized Data and Reconstructing Tensors\n",
    "\n",
    "This cell demonstrates how to load the Parquet file saved above and convert the `input_ids_list`, `attention_mask_list`, and `labels` back into PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2622362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame reloaded and PyTorch tensors reconstructed:\n",
      "                                                text  \\\n",
      "0  Опитът на колективния Запад да „обезкърви Руси...   \n",
      "1  Цончо Ганев, “Възраждане”: Обещали сме на Укра...   \n",
      "2  Подкрепата за Киев от страна на Запада вече не...   \n",
      "3  Дмитрий Медведев: НПО-та, спонсорирани от Соро...   \n",
      "4  Британски дипломат обвини Запада за украинския...   \n",
      "\n",
      "                                           labels_pt  \\\n",
      "0  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
      "1  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
      "2  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
      "3  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
      "4  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
      "\n",
      "                                        input_ids_pt  \\\n",
      "0  [tensor(0), tensor(1089), tensor(22617), tenso...   \n",
      "1  [tensor(0), tensor(160480), tensor(108723), te...   \n",
      "2  [tensor(0), tensor(10405), tensor(24724), tens...   \n",
      "3  [tensor(0), tensor(154888), tensor(189322), te...   \n",
      "4  [tensor(0), tensor(57172), tensor(1707), tenso...   \n",
      "\n",
      "                                   attention_mask_pt  \n",
      "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
      "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
      "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
      "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
      "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
      "\n",
      "First sample's reconstructed tensors:\n",
      "Input IDs shape: torch.Size([1024]) dtype: torch.int64\n",
      "Attention Mask shape: torch.Size([1024]) dtype: torch.int64\n",
      "Labels shape: torch.Size([117]) dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "input_parquet_path_tokenized = 'data/processed/phase0_baseline_tokenized_and_labeled.parquet'\n",
    "\n",
    "loaded_df = pd.read_parquet(input_parquet_path_tokenized)\n",
    "\n",
    "loaded_df['input_ids_pt'] = loaded_df['input_ids_list'].apply(lambda x: torch.tensor(x, dtype=torch.long))\n",
    "loaded_df['attention_mask_pt'] = loaded_df['attention_mask_list'].apply(lambda x: torch.tensor(x, dtype=torch.long))\n",
    "loaded_df['labels_pt'] = loaded_df['labels'].apply(lambda x: torch.tensor(x, dtype=torch.float))\n",
    "\n",
    "print(\"DataFrame reloaded and PyTorch tensors reconstructed:\")\n",
    "print(loaded_df[['text', 'labels_pt', 'input_ids_pt', 'attention_mask_pt']].head())\n",
    "\n",
    "if not loaded_df.empty:\n",
    "    sample_input_ids = loaded_df['input_ids_pt'].iloc[0]\n",
    "    sample_attention_mask = loaded_df['attention_mask_pt'].iloc[0]\n",
    "    sample_labels = loaded_df['labels_pt'].iloc[0]\n",
    "    print(\"\\nFirst sample's reconstructed tensors:\")\n",
    "    print(\"Input IDs shape:\", sample_input_ids.shape, \"dtype:\", sample_input_ids.dtype)\n",
    "    print(\"Attention Mask shape:\", sample_attention_mask.shape, \"dtype:\", sample_attention_mask.dtype)\n",
    "    print(\"Labels shape:\", sample_labels.shape, \"dtype:\", sample_labels.dtype)\n",
    "else:\n",
    "    print(\"\\nLoaded DataFrame is empty.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
