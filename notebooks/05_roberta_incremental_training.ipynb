{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d57e4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "# Set the current working directory to the project root\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc074bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "optimal_threshold = 0.81\n",
    "PATH_TO_BEST_MODEL = 'models/phase0_xlmr_best_model.bin'\n",
    "MODEL_NAME = 'xlm-roberta-base'\n",
    "CONTINUAL_LEARNING_MODEL_PATH = 'models/phase0_xlmr_continual_learning_model.bin'\n",
    "CONTINUAL_LEARNING_EPOCHS = 5\n",
    "CONTINUAL_LEARNING_LR = 2e-6 \n",
    "CONTINUAL_LEARNING_PATIENCE = 2\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "H_LAMBDA = 1.5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ed8d97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/twoface/Documents/Passau/masterarbeit/hybrid-text-classification/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing original training data ---\n",
      "Loading annotations and taxonomy...\n",
      "Mapping labels to IDs and creating binarized vectors...\n",
      "Splitting dataset into train, validation, and test sets...\n",
      "Dataset split sizes: Train=987, Validation=341, Test=371\n",
      "Loading tokenizer ('xlm-roberta-base') and creating PyTorch datasets...\n",
      "Data preparation complete.\n",
      "\n",
      "Original datasets created:\n",
      "  - Original Train set size: 987\n",
      "  - Original Validation set size: 341\n",
      "  - Original Test set size: 371\n",
      "--- Preparing incremental training data from devset ---\n",
      "Loading annotations and taxonomy...\n",
      "Mapping labels to IDs and creating binarized vectors...\n",
      "Splitting dataset into train, validation, and test sets...\n",
      "Dataset split sizes: Train=99, Validation=40, Test=39\n",
      "Loading tokenizer ('xlm-roberta-base') and creating PyTorch datasets...\n",
      "Data preparation complete.\n",
      "\n",
      "Incremental datasets created:\n",
      "  - Incremental Train set size: 99\n",
      "  - Incremental Validation set size: 40\n",
      "  - Incremental Test set size: 39\n"
     ]
    }
   ],
   "source": [
    "from src.scripts.data_preparation import prepare_datasets\n",
    "\n",
    "print(\"--- Preparing original training data ---\")\n",
    "\n",
    "(\n",
    "    original_train_dataset,\n",
    "    original_val_dataset,\n",
    "    original_test_dataset,\n",
    "    tokenizer, \n",
    "    id_to_label, \n",
    "    label_to_id,\n",
    "    parent_child_pairs, \n",
    "    num_total_labels, \n",
    ") = prepare_datasets(\n",
    "    data_folder='data',\n",
    "    model_name=MODEL_NAME,\n",
    "    docs_folder='raw-documents'\n",
    ")\n",
    "\n",
    "print(\"\\nOriginal datasets created:\")\n",
    "print(f\"  - Original Train set size: {len(original_train_dataset)}\")\n",
    "print(f\"  - Original Validation set size: {len(original_val_dataset)}\")\n",
    "print(f\"  - Original Test set size: {len(original_test_dataset)}\")\n",
    "\n",
    "print(\"--- Preparing incremental training data from devset ---\")\n",
    "# We can reuse the same model name and max length from the initial setup.\n",
    "# The tokenizer is already loaded, but prepare_datasets will load it again.\n",
    "# This is okay for this demonstration.\n",
    "(\n",
    "    inc_train_dataset,\n",
    "    inc_val_dataset,\n",
    "    inc_test_dataset,\n",
    "    _, # tokenizer - assuming it's the same\n",
    "    _, # id_to_label - assuming it's the same\n",
    "    _,\n",
    "    _, # parent_child_pairs - assuming they are the same\n",
    "    _, # num_total_labels - assuming it's the same\n",
    ") = prepare_datasets(\n",
    "    data_folder='devset',\n",
    "    model_name=MODEL_NAME,\n",
    "    docs_folder='subtask-2-documents'\n",
    ")\n",
    "\n",
    "print(\"\\nIncremental datasets created:\")\n",
    "print(f\"  - Incremental Train set size: {len(inc_train_dataset)}\")\n",
    "print(f\"  - Incremental Validation set size: {len(inc_val_dataset)}\")\n",
    "print(f\"  - Incremental Test set size: {len(inc_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b6dad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Combined Training Dataset ---\n",
      "Original training set size: 987\n",
      "Incremental train set size: 99\n",
      "Incremental validation set size: 40\n",
      "Incremental test set size: 39\n",
      "Total combined training set size: 1165\n",
      "\n",
      "--- Evaluation Datasets ---\n",
      "Original validation set size: 341\n",
      "Original test set size: 371\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# Combine the original training data with all parts of the incremental data\n",
    "combined_train_dataset = ConcatDataset([\n",
    "    original_train_dataset,\n",
    "    inc_train_dataset,\n",
    "    inc_val_dataset,\n",
    "    inc_test_dataset\n",
    "])\n",
    "\n",
    "print(f\"--- Combined Training Dataset ---\")\n",
    "print(f\"Original training set size: {len(original_train_dataset)}\")\n",
    "print(f\"Incremental train set size: {len(inc_train_dataset)}\")\n",
    "print(f\"Incremental validation set size: {len(inc_val_dataset)}\")\n",
    "print(f\"Incremental test set size: {len(inc_test_dataset)}\")\n",
    "print(f\"Total combined training set size: {len(combined_train_dataset)}\")\n",
    "\n",
    "# The original validation and test sets remain unchanged for final evaluation\n",
    "print(f\"\\n--- Evaluation Datasets ---\")\n",
    "print(f\"Original validation set size: {len(original_val_dataset)}\")\n",
    "print(f\"Original test set size: {len(original_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57fdee50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=117, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from src.training.engine import train_epoch, evaluate\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_total_labels,\n",
    "    problem_type='multi_label_classification',\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "model.load_state_dict(torch.load(PATH_TO_BEST_MODEL))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3f404a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    combined_train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    original_val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    original_test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ded0b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer set up successfully.\n",
      "Scheduler set up successfully.\n"
     ]
    }
   ],
   "source": [
    "from src.training.setup import setup_optimizer_and_scheduler\n",
    "\n",
    "\n",
    "optimizer, scheduler = setup_optimizer_and_scheduler(\n",
    "    model=model,\n",
    "    learning_rate=CONTINUAL_LEARNING_LR,\n",
    "    epochs=CONTINUAL_LEARNING_EPOCHS,\n",
    "    train_dataloader=train_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9054d755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continual Learning Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [02:49<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.3078\n",
      "Running evaluation on the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 22/22 [00:06<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.2728\n",
      "Validation F1-score (micro): 0.1383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continual Learning Epochs:  20%|██        | 1/5 [02:57<11:51, 177.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved.\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [02:36<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2712\n",
      "Running evaluation on the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 22/22 [00:06<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.2585\n",
      "Validation F1-score (micro): 0.1305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continual Learning Epochs:  40%|████      | 2/5 [05:43<08:31, 170.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved.\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [03:15<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2591\n",
      "Running evaluation on the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 22/22 [01:27<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.2496\n",
      "Validation F1-score (micro): 0.1180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continual Learning Epochs:  60%|██████    | 3/5 [10:27<07:24, 222.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved.\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [11:44<00:00,  9.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2529\n",
      "Running evaluation on the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 22/22 [01:06<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.2449\n",
      "Validation F1-score (micro): 0.1119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continual Learning Epochs:  80%|████████  | 4/5 [23:20<07:19, 439.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved.\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [06:38<00:00,  5.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2489\n",
      "Running evaluation on the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 22/22 [01:03<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.2433\n",
      "Validation F1-score (micro): 0.1132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continual Learning Epochs: 100%|██████████| 5/5 [31:04<00:00, 372.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "patience_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(CONTINUAL_LEARNING_EPOCHS), desc=\"Continual Learning Epochs\"):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{CONTINUAL_LEARNING_EPOCHS}\")\n",
    "    \n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss_function,\n",
    "        device,\n",
    "        parent_child_pairs,\n",
    "        H_LAMBDA\n",
    "    )\n",
    "    \n",
    "    val_loss, metrics = evaluate(\n",
    "        model,\n",
    "        val_dataloader,\n",
    "        loss_function,\n",
    "        device,\n",
    "        H_LAMBDA,\n",
    "        parent_child_pairs,\n",
    "        threshold=optimal_threshold\n",
    "    )\n",
    "    \n",
    "    print(f\"Validation F1-score (micro): {metrics['f1_micro']:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), CONTINUAL_LEARNING_MODEL_PATH)\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= CONTINUAL_LEARNING_PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b272522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading best model for threshold finding and final evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Raw Predictions: 100%|██████████| 22/22 [01:02<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for the best threshold to optimize f1_micro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching Thresholds: 100%|██████████| 81/81 [00:00<00:00, -189.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search complete!\n",
      "Best Threshold found: 0.87\n",
      "Best Validation F1_micro: 0.3913\n",
      "\n",
      "--- Final Evaluation on TEST set using the Optimal Threshold ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Raw Predictions: 100%|██████████| 24/24 [01:12<00:00,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Reportable Performance on Test Set:\n",
      "  - F1 Micro: 0.3406\n",
      "  - F1 Macro: 0.0724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- Import your new and existing functions ---\n",
    "# Your existing compute_metrics is inside engine.py\n",
    "from src.training.engine import get_raw_predictions, compute_metrics\n",
    "from src.utils.metrics import find_best_threshold\n",
    "\n",
    "# --- Assumed objects are available ---\n",
    "# model, val_dataloader, test_dataloader, device, parent_child_pairs\n",
    "MODEL_OUTPUT_PATH = \"models/phase0_xlmr_best_model.bin\"\n",
    "\n",
    "# 1. LOAD THE BEST MODEL WEIGHTS SAVED DURING TRAINING\n",
    "print(\"\\n--- Loading best model for threshold finding and final evaluation ---\")\n",
    "model.load_state_dict(torch.load(MODEL_OUTPUT_PATH))\n",
    "\n",
    "model.to(device) # Make sure model is on the correct device\n",
    "\n",
    "# 2. GET PREDICTIONS ON THE VALIDATION SET\n",
    "# Use the new, clean function from engine.py\n",
    "val_logits, val_true_labels = get_raw_predictions(model, val_dataloader, device)\n",
    "\n",
    "# 3. FIND THE OPTIMAL THRESHOLD\n",
    "# Use the new function from metrics.py\n",
    "optimal_threshold = find_best_threshold(\n",
    "    val_logits,\n",
    "    val_true_labels,\n",
    "    parent_child_pairs,\n",
    "    metric_to_optimize='f1_micro',\n",
    "    compute_metrics_fn=compute_metrics # Pass your metrics function\n",
    ")\n",
    "\n",
    "# 4. FINAL EVALUATION ON THE UNSEEN TEST SET\n",
    "print(\"\\n--- Final Evaluation on TEST set using the Optimal Threshold ---\")\n",
    "\n",
    "# Get raw predictions for the test set\n",
    "test_logits, test_true_labels = get_raw_predictions(model, test_dataloader, device)\n",
    "\n",
    "# Calculate final metrics using your original compute_metrics function\n",
    "# and the optimal_threshold you just found\n",
    "final_metrics = compute_metrics(\n",
    "    test_logits,\n",
    "    test_true_labels,\n",
    "    parent_child_pairs,\n",
    "    threshold=optimal_threshold\n",
    ")\n",
    "\n",
    "print(f\"Final Reportable Performance on Test Set:\")\n",
    "print(f\"  - F1 Micro: {final_metrics['f1_micro']:.4f}\")\n",
    "print(f\"  - F1 Macro: {final_metrics['f1_macro']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4325a0",
   "metadata": {},
   "source": [
    "# Finding multilevel thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "284b695e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 narrative-level labels.\n",
      "Found 95 sub-narrative-level labels.\n"
     ]
    }
   ],
   "source": [
    "narrative_indices = [\n",
    "    idx for idx in range(num_total_labels) \n",
    "    if id_to_label[idx].count(':') == 1\n",
    "]\n",
    "\n",
    "subnarrative_indices = [\n",
    "    idx for idx in range(num_total_labels)\n",
    "    if id_to_label[idx].count(':') == 2\n",
    "]\n",
    "\n",
    "print(f\"Found {len(narrative_indices)} narrative-level labels.\")\n",
    "print(f\"Found {len(subnarrative_indices)} sub-narrative-level labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03859472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting raw logits from validation set to find best thresholds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Raw Predictions: 100%|██████████| 22/22 [00:07<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting logits to probabilities...\n",
      "--- Searching for optimal NARRATIVE threshold (optimizing F1 Micro) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Narrative Thresholds: 100%|██████████| 81/81 [00:00<00:00, 671.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Narrative Threshold: 0.89 (F1 Micro: 0.4859)\n",
      "\n",
      "--- Searching for optimal SUB-NARRATIVE threshold (optimizing F1 Micro) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sub-narrative Thresholds: 100%|██████████| 81/81 [00:00<00:00, 467.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Sub-narrative Threshold: 0.80 (F1 Micro: 0.2863)\n",
      "\n",
      "--- Calculating overall F1 score with per-level thresholds ---\n",
      "Overall F1 Micro with combined per-level thresholds: 0.3656\n",
      "\n",
      "--- Final Evaluation on TEST set using Per-Level Thresholds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Raw Predictions: 100%|██████████| 23/23 [00:05<00:00,  4.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.utils.metrics import find_per_level_thresholds\n",
    "from src.training.engine import get_raw_predictions\n",
    "\n",
    "\n",
    "print(\"Getting raw logits from validation set to find best thresholds...\")\n",
    "val_logits, val_true_labels = get_raw_predictions(model, val_dataloader, device)\n",
    "\n",
    "print(\"Converting logits to probabilities...\")\n",
    "val_probabilities = 1 / (1 + np.exp(-val_logits))\n",
    "\n",
    "threshold_results = find_per_level_thresholds(\n",
    "    val_probabilities,\n",
    "    val_true_labels,\n",
    "    narrative_indices,\n",
    "    subnarrative_indices,\n",
    "    parent_child_pairs\n",
    ")\n",
    "\n",
    "optimal_narr_thresh = threshold_results['narrative_threshold']\n",
    "optimal_subnarr_thresh = threshold_results['subnarrative_threshold']\n",
    "\n",
    "print(\"\\n--- Final Evaluation on TEST set using Per-Level Thresholds ---\")\n",
    "\n",
    "# Get raw logits for the test set\n",
    "test_logits, test_true_labels = get_raw_predictions(model, test_dataloader, device)\n",
    "\n",
    "# Convert test logits to probabilities\n",
    "test_probabilities = 1 / (1 + np.exp(-test_logits))\n",
    "\n",
    "# Apply the two different thresholds to the test probabilities\n",
    "final_test_preds = np.zeros_like(test_probabilities, dtype=int)\n",
    "final_test_preds[:, narrative_indices] = (test_probabilities[:, narrative_indices] > optimal_narr_thresh).astype(int)\n",
    "final_test_preds[:, subnarrative_indices] = (test_probabilities[:, subnarrative_indices] > optimal_subnarr_thresh).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688899cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.engine import compute_metrics\n",
    "\n",
    "# Evaluate the final_test_preds (using per-level thresholds) against the true test labels\n",
    "final_per_level_metrics = compute_metrics(\n",
    "    final_test_preds,\n",
    "    test_true_labels,\n",
    "    parent_child_pairs,\n",
    "    threshold=None  # Already thresholded predictions\n",
    ")\n",
    "\n",
    "print(\"Test set results using per-level thresholds:\")\n",
    "print(f\"  - F1 Micro: {final_per_level_metrics['f1_micro']:.4f}\")\n",
    "print(f\"  - F1 Macro: {final_per_level_metrics['f1_macro']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
