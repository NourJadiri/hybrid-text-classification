{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d57e4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "# Set the current working directory to the project root\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc074bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "optimal_threshold = 0.81\n",
    "PATH_TO_BEST_MODEL = 'models/phase0_xlmr_best_model.bin'\n",
    "MODEL_NAME = 'xlm-roberta-base'\n",
    "CONTINUAL_LEARNING_MODEL_PATH = 'models/phase0_xlmr_continual_learning_model.bin'\n",
    "CONTINUAL_LEARNING_EPOCHS = 5\n",
    "CONTINUAL_LEARNING_LR = 2e-6 \n",
    "CONTINUAL_LEARNING_PATIENCE = 2\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "H_LAMBDA = 1.5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed8d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.data_preparation import prepare_datasets\n",
    "\n",
    "print(\"--- Preparing original training data ---\")\n",
    "\n",
    "(\n",
    "    original_train_dataset,\n",
    "    original_val_dataset,\n",
    "    original_test_dataset,\n",
    "    tokenizer, \n",
    "    id_to_label, \n",
    "    label_to_id,\n",
    "    parent_child_pairs, \n",
    "    num_total_labels, \n",
    ") = prepare_datasets(\n",
    "    data_folder='data',\n",
    "    model_name=MODEL_NAME,\n",
    "    docs_folder='raw-documents'\n",
    ")\n",
    "\n",
    "print(\"\\nOriginal datasets created:\")\n",
    "print(f\"  - Original Train set size: {len(original_train_dataset)}\")\n",
    "print(f\"  - Original Validation set size: {len(original_val_dataset)}\")\n",
    "print(f\"  - Original Test set size: {len(original_test_dataset)}\")\n",
    "\n",
    "print(\"--- Preparing incremental training data from devset ---\")\n",
    "# We can reuse the same model name and max length from the initial setup.\n",
    "# The tokenizer is already loaded, but prepare_datasets will load it again.\n",
    "# This is okay for this demonstration.\n",
    "(\n",
    "    inc_train_dataset,\n",
    "    inc_val_dataset,\n",
    "    inc_test_dataset,\n",
    "    _, # tokenizer - assuming it's the same\n",
    "    _, # id_to_label - assuming it's the same\n",
    "    _,\n",
    "    _, # parent_child_pairs - assuming they are the same\n",
    "    _, # num_total_labels - assuming it's the same\n",
    ") = prepare_datasets(\n",
    "    data_folder='devset',\n",
    "    model_name=MODEL_NAME,\n",
    "    docs_folder='subtask-2-documents'\n",
    ")\n",
    "\n",
    "print(\"\\nIncremental datasets created:\")\n",
    "print(f\"  - Incremental Train set size: {len(inc_train_dataset)}\")\n",
    "print(f\"  - Incremental Validation set size: {len(inc_val_dataset)}\")\n",
    "print(f\"  - Incremental Test set size: {len(inc_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6dad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# Combine the original training data with all parts of the incremental data\n",
    "combined_train_dataset = ConcatDataset([\n",
    "    original_train_dataset,\n",
    "    inc_train_dataset,\n",
    "    inc_val_dataset,\n",
    "    inc_test_dataset\n",
    "])\n",
    "\n",
    "print(f\"--- Combined Training Dataset ---\")\n",
    "print(f\"Original training set size: {len(original_train_dataset)}\")\n",
    "print(f\"Incremental train set size: {len(inc_train_dataset)}\")\n",
    "print(f\"Incremental validation set size: {len(inc_val_dataset)}\")\n",
    "print(f\"Incremental test set size: {len(inc_test_dataset)}\")\n",
    "print(f\"Total combined training set size: {len(combined_train_dataset)}\")\n",
    "\n",
    "# The original validation and test sets remain unchanged for final evaluation\n",
    "print(f\"\\n--- Evaluation Datasets ---\")\n",
    "print(f\"Original validation set size: {len(original_val_dataset)}\")\n",
    "print(f\"Original test set size: {len(original_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fdee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from src.training.engine import train_epoch, evaluate\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_total_labels,\n",
    "    problem_type='multi_label_classification',\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "model.load_state_dict(torch.load(PATH_TO_BEST_MODEL))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f404a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    combined_train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    original_val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    original_test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded0b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.setup import setup_optimizer_and_scheduler\n",
    "\n",
    "\n",
    "optimizer, scheduler = setup_optimizer_and_scheduler(\n",
    "    model=model,\n",
    "    learning_rate=CONTINUAL_LEARNING_LR,\n",
    "    epochs=CONTINUAL_LEARNING_EPOCHS,\n",
    "    train_dataloader=train_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "patience_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(CONTINUAL_LEARNING_EPOCHS), desc=\"Continual Learning Epochs\"):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{CONTINUAL_LEARNING_EPOCHS}\")\n",
    "    \n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss_function,\n",
    "        device,\n",
    "        parent_child_pairs,\n",
    "        H_LAMBDA\n",
    "    )\n",
    "    \n",
    "    val_loss, metrics = evaluate(\n",
    "        model,\n",
    "        val_dataloader,\n",
    "        loss_function,\n",
    "        device,\n",
    "        H_LAMBDA,\n",
    "        parent_child_pairs,\n",
    "        threshold=optimal_threshold\n",
    "    )\n",
    "    \n",
    "    print(f\"Validation F1-score (micro): {metrics['f1_micro']:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), CONTINUAL_LEARNING_MODEL_PATH)\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= CONTINUAL_LEARNING_PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b272522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- Import your new and existing functions ---\n",
    "# Your existing compute_metrics is inside engine.py\n",
    "from src.training.engine import get_raw_predictions, compute_metrics\n",
    "from src.utils.metrics import find_best_threshold\n",
    "\n",
    "# --- Assumed objects are available ---\n",
    "# model, val_dataloader, test_dataloader, device, parent_child_pairs\n",
    "MODEL_OUTPUT_PATH = \"models/phase0_xlmr_best_model.bin\"\n",
    "\n",
    "# 1. LOAD THE BEST MODEL WEIGHTS SAVED DURING TRAINING\n",
    "print(\"\\n--- Loading best model for threshold finding and final evaluation ---\")\n",
    "model.load_state_dict(torch.load(MODEL_OUTPUT_PATH))\n",
    "\n",
    "model.to(device) # Make sure model is on the correct device\n",
    "\n",
    "# 2. GET PREDICTIONS ON THE VALIDATION SET\n",
    "# Use the new, clean function from engine.py\n",
    "val_logits, val_true_labels = get_raw_predictions(model, val_dataloader, device)\n",
    "\n",
    "# 3. FIND THE OPTIMAL THRESHOLD\n",
    "# Use the new function from metrics.py\n",
    "optimal_threshold = find_best_threshold(\n",
    "    val_logits,\n",
    "    val_true_labels,\n",
    "    parent_child_pairs,\n",
    "    metric_to_optimize='f1_micro',\n",
    "    compute_metrics_fn=compute_metrics # Pass your metrics function\n",
    ")\n",
    "\n",
    "# 4. FINAL EVALUATION ON THE UNSEEN TEST SET\n",
    "print(\"\\n--- Final Evaluation on TEST set using the Optimal Threshold ---\")\n",
    "\n",
    "# Get raw predictions for the test set\n",
    "test_logits, test_true_labels = get_raw_predictions(model, test_dataloader, device)\n",
    "\n",
    "# Calculate final metrics using your original compute_metrics function\n",
    "# and the optimal_threshold you just found\n",
    "final_metrics = compute_metrics(\n",
    "    test_logits,\n",
    "    test_true_labels,\n",
    "    parent_child_pairs,\n",
    "    threshold=optimal_threshold\n",
    ")\n",
    "\n",
    "print(f\"Final Reportable Performance on Test Set:\")\n",
    "print(f\"  - F1 Micro: {final_metrics['f1_micro']:.4f}\")\n",
    "print(f\"  - F1 Macro: {final_metrics['f1_macro']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4325a0",
   "metadata": {},
   "source": [
    "# Finding multilevel thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "narrative_indices = [\n",
    "    idx for idx in range(num_total_labels) \n",
    "    if id_to_label[idx].count(':') == 1\n",
    "]\n",
    "\n",
    "subnarrative_indices = [\n",
    "    idx for idx in range(num_total_labels)\n",
    "    if id_to_label[idx].count(':') == 2\n",
    "]\n",
    "\n",
    "print(f\"Found {len(narrative_indices)} narrative-level labels.\")\n",
    "print(f\"Found {len(subnarrative_indices)} sub-narrative-level labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03859472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.metrics import find_per_level_thresholds\n",
    "from src.training.engine import get_raw_predictions\n",
    "\n",
    "\n",
    "print(\"Getting raw logits from validation set to find best thresholds...\")\n",
    "val_logits, val_true_labels = get_raw_predictions(model, val_dataloader, device)\n",
    "\n",
    "print(\"Converting logits to probabilities...\")\n",
    "val_probabilities = 1 / (1 + np.exp(-val_logits))\n",
    "\n",
    "threshold_results = find_per_level_thresholds(\n",
    "    val_probabilities,\n",
    "    val_true_labels,\n",
    "    narrative_indices,\n",
    "    subnarrative_indices,\n",
    "    parent_child_pairs\n",
    ")\n",
    "\n",
    "optimal_narr_thresh = threshold_results['narrative_threshold']\n",
    "optimal_subnarr_thresh = threshold_results['subnarrative_threshold']\n",
    "\n",
    "print(\"\\n--- Final Evaluation on TEST set using Per-Level Thresholds ---\")\n",
    "\n",
    "# Get raw logits for the test set\n",
    "test_logits, test_true_labels = get_raw_predictions(model, test_dataloader, device)\n",
    "\n",
    "# Convert test logits to probabilities\n",
    "test_probabilities = 1 / (1 + np.exp(-test_logits))\n",
    "\n",
    "# Apply the two different thresholds to the test probabilities\n",
    "final_test_preds = np.zeros_like(test_probabilities, dtype=int)\n",
    "final_test_preds[:, narrative_indices] = (test_probabilities[:, narrative_indices] > optimal_narr_thresh).astype(int)\n",
    "final_test_preds[:, subnarrative_indices] = (test_probabilities[:, subnarrative_indices] > optimal_subnarr_thresh).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688899cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.engine import compute_metrics\n",
    "\n",
    "# Evaluate the final_test_preds (using per-level thresholds) against the true test labels\n",
    "final_per_level_metrics = compute_metrics(\n",
    "    final_test_preds,\n",
    "    test_true_labels,\n",
    "    parent_child_pairs,\n",
    "    threshold=None  # Already thresholded predictions\n",
    ")\n",
    "\n",
    "print(\"Test set results using per-level thresholds:\")\n",
    "print(f\"  - F1 Micro: {final_per_level_metrics['f1_micro']:.4f}\")\n",
    "print(f\"  - F1 Macro: {final_per_level_metrics['f1_macro']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
