{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b36ae2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "# Set the current working directory to the project root\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d113eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_management.loaders import load_labeled_df\n",
    "\n",
    "\n",
    "dataset = load_labeled_df(\"phase0_baseline_labeled.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c7888",
   "metadata": {},
   "source": [
    "# Finetuning the Qwenner for the narratives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ee6a54",
   "metadata": {},
   "source": [
    "First let's start by taking only the columns that interest us from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4474f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "narratives_dataset = dataset[[\"text\", \"narratives\"]].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1981a1d5",
   "metadata": {},
   "source": [
    "Defining the model we are about to finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2564eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unsloth/Qwen3-4B-Instruct-2507-unsloth-bnb-4bit\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfba565",
   "metadata": {},
   "source": [
    "Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0bee261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.9: Fast Qwen3 patching. Transformers: 4.55.4.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Laptop GPU. Num GPUs = 1. Max memory: 7.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "487244d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc2184b",
   "metadata": {},
   "source": [
    "We will need to use the chat template of qwen, so we import the function from unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f61bbffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"qwen3-instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4728badd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1699, 2)\n",
      "\n",
      "First few rows:\n",
      "                                                text  \\\n",
      "0  Опитът на колективния Запад да „обезкърви Руси...   \n",
      "1  Цончо Ганев, “Възраждане”: Обещали сме на Укра...   \n",
      "2  Подкрепата за Киев от страна на Запада вече не...   \n",
      "3  Дмитрий Медведев: НПО-та, спонсорирани от Соро...   \n",
      "4  Британски дипломат обвини Запада за украинския...   \n",
      "\n",
      "                                          narratives  \n",
      "0  [URW: Blaming the war on others rather than th...  \n",
      "1                        [URW: Discrediting Ukraine]  \n",
      "2  [URW: Discrediting the West, Diplomacy, URW: D...  \n",
      "3  [URW: Discrediting the West, Diplomacy, URW: D...  \n",
      "4  [URW: Discrediting the West, Diplomacy, URW: P...  \n",
      "\n",
      "Column info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1699 entries, 0 to 1698\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   text        1699 non-null   object\n",
      " 1   narratives  1699 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 26.7+ KB\n",
      "None\n",
      "\n",
      "Unique narratives:\n",
      "narratives\n",
      "[Other]                                                                                                                                                                                         324\n",
      "[CC: Amplifying Climate Fears]                                                                                                                                                                  108\n",
      "[URW: Praise of Russia]                                                                                                                                                                          65\n",
      "[URW: Discrediting Ukraine]                                                                                                                                                                      61\n",
      "[URW: Amplifying war-related fears]                                                                                                                                                              44\n",
      "                                                                                                                                                                                               ... \n",
      "[CC: Criticism of institutions and authorities, CC: Criticism of climate policies]                                                                                                                1\n",
      "[CC: Criticism of climate movement, CC: Criticism of institutions and authorities]                                                                                                                1\n",
      "[URW: Discrediting the West, Diplomacy, URW: Negative Consequences for the West]                                                                                                                  1\n",
      "[URW: Discrediting the West, Diplomacy, URW: Discrediting the West, Diplomacy]                                                                                                                    1\n",
      "[URW: Distrust towards Media, URW: Speculating war outcomes, URW: Discrediting the West, Diplomacy, URW: Discrediting Ukraine, URW: Praise of Russia, URW: Discrediting the West, Diplomacy]      1\n",
      "Name: count, Length: 939, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the dataset structure\n",
    "print(\"Dataset shape:\", narratives_dataset.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(narratives_dataset.head())\n",
    "print(\"\\nColumn info:\")\n",
    "print(narratives_dataset.info())\n",
    "print(\"\\nUnique narratives:\")\n",
    "print(narratives_dataset['narratives'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f093bc",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation for Chat Format\n",
    "\n",
    "Now we need to convert our classification dataset into a chat format that Qwen can understand. We'll create instruction-response pairs where:\n",
    "- **Instruction**: Ask the model to classify the text\n",
    "- **Response**: The expected narrative labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "993d7794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chat format:\n",
      "{'messages': [{'role': 'user', 'content': 'You are an expert at analyzing text for propaganda narratives. Your task is to classify the given text and identify which narratives are present.\\n\\nAvailable narrative categories include:\\n- URW (Ukraine, Russia, War related): Various subcategories like \"Discrediting Ukraine\", \"Praise of Russia\", \"Discrediting the West\", etc.\\n- CC (Climate Change): Various subcategories like \"Amplifying Climate Fears\", \"Criticism of climate policies\", etc.\\n- Other: For texts that don\\'t fit the main categories\\n\\nPlease analyze the following text and identify all relevant narratives present:\\n\\nText: Опитът на колективния Запад да „обезкърви Русия“ с ръцете на властите в Киев „се провали с гръм и трясък“ и скоро от Украйна ...\\n\\nОпитът на колективния Запад да „обезкърви Русия“ с ръцете на властите в Киев „се провали с гръм и трясък“ и скоро от Украйна няма да остане почти нищо, ако не започне процесът на разрешаване на този въоръжен конфликт. Тази гледна точка изрази пред ТАСС бившият началник на кабинета на държавния секретар на САЩ Колин Пауъл, пенсионирания полковник от армията на САЩ Лорънс Уилкерсън.\\n\"Подкрепата на САЩ, НАТО и други западни съюзници за войната в Украйна срещу Русия е безумна. Това води до смъртта на украински войници в името на загубена кауза, ако не вземете предвид печеленето на пари от американски и европейски военни изпълнители, както и бруталния опит да се обезкърви Русия чрез трети страни“, каза Уилкерсън.\\n\"Усилията за постигане на тази последна цел, при цялата ѝ подлост, се провалиха с гръм и трясък\", заяви видният американски военен анализатор. - В действителност Русия очевидно побеждава\". Ако скоро не бъде обявено прекратяване на огъня и не бъде свикана истинска мирна конференция, тогава от Украйна практически нищо няма да остане“, убеден е Уилкерсън. Той спомена \"истинска мирна конференция\" в противовес на срещата в Бюргенсток (Швейцария), която беше свикана на 15-16 юни по инициатива на Запада и на която Русия не беше поканена. Дори много западни политически анализатори заявиха след срещата, че тя е претърпяла фиаско. Нито една страна от БРИКС не подкрепи заключителния документ от срещата.'}, {'role': 'assistant', 'content': 'URW: Blaming the war on others rather than the invader, URW: Discrediting the West, Diplomacy, URW: Discrediting the West, Diplomacy, URW: Amplifying war-related fears'}]}\n"
     ]
    }
   ],
   "source": [
    "def create_chat_format(text, narratives):\n",
    "    \"\"\"Convert a text-narrative pair into chat format for finetuning\"\"\"\n",
    "    \n",
    "    # Create the instruction prompt\n",
    "    instruction = \"\"\"You are an expert at analyzing text for propaganda narratives. Your task is to classify the given text and identify which narratives are present.\n",
    "\n",
    "Available narrative categories include:\n",
    "- URW (Ukraine, Russia, War related): Various subcategories like \"Discrediting Ukraine\", \"Praise of Russia\", \"Discrediting the West\", etc.\n",
    "- CC (Climate Change): Various subcategories like \"Amplifying Climate Fears\", \"Criticism of climate policies\", etc.\n",
    "- Other: For texts that don't fit the main categories\n",
    "\n",
    "Please analyze the following text and identify all relevant narratives present:\"\"\"\n",
    "\n",
    "    # Format the expected response\n",
    "    if isinstance(narratives, str):\n",
    "        # Parse the string representation of list\n",
    "        import ast\n",
    "        try:\n",
    "            narratives_list = ast.literal_eval(narratives)\n",
    "        except:\n",
    "            narratives_list = [narratives]\n",
    "    else:\n",
    "        narratives_list = narratives\n",
    "    \n",
    "    response = \", \".join(narratives_list)\n",
    "    \n",
    "    # Create the chat format\n",
    "    chat_data = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": f\"{instruction}\\n\\nText: {text}\"},\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return chat_data\n",
    "\n",
    "# Test the function with the first example\n",
    "sample_chat = create_chat_format(\n",
    "    narratives_dataset.iloc[0]['text'], \n",
    "    narratives_dataset.iloc[0]['narratives']\n",
    ")\n",
    "print(\"Sample chat format:\")\n",
    "print(sample_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1defc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting dataset to chat format...\n",
      "Converted 1699 samples to chat format\n",
      "Training samples: 1529\n",
      "Validation samples: 170\n",
      "\n",
      "Sample training example:\n",
      "User: You are an expert at analyzing text for propaganda narratives. Your task is to classify the given text and identify which narratives are present.\n",
      "\n",
      "Available narrative categories include:\n",
      "- URW (Ukrain...\n",
      "Assistant: URW: Speculating war outcomes, URW: Discrediting the West, Diplomacy, URW: Discrediting Ukraine, URW: Discrediting Ukraine, URW: Russia is the Victim, URW: Discrediting Ukraine\n"
     ]
    }
   ],
   "source": [
    "# Convert the entire dataset to chat format\n",
    "print(\"Converting dataset to chat format...\")\n",
    "chat_dataset = []\n",
    "\n",
    "for idx, row in narratives_dataset.iterrows():\n",
    "    chat_data = create_chat_format(row['text'], row['narratives'])\n",
    "    chat_dataset.append(chat_data)\n",
    "\n",
    "print(f\"Converted {len(chat_dataset)} samples to chat format\")\n",
    "\n",
    "# Split the data into train and validation sets (simple random split)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(\n",
    "    chat_dataset, \n",
    "    test_size=0.1, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "\n",
    "# Show a sample from training data\n",
    "print(\"\\nSample training example:\")\n",
    "print(\"User:\", train_data[0]['messages'][0]['content'][:200] + \"...\")\n",
    "print(\"Assistant:\", train_data[0]['messages'][1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45021515",
   "metadata": {},
   "source": [
    "## Step 2: Convert to Unsloth Format\n",
    "\n",
    "Now we need to convert our chat data to the format expected by unsloth's training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "976faa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created training and validation datasets\n",
      "Training dataset: Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 1529\n",
      "})\n",
      "Validation dataset: Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 170\n",
      "})\n",
      "\n",
      "Sample formatted text:\n",
      "<|im_start|>user\n",
      "You are an expert at analyzing text for propaganda narratives. Your task is to classify the given text and identify which narratives are present.\n",
      "\n",
      "Available narrative categories include:\n",
      "- URW (Ukraine, Russia, War related): Various subcategories like \"Discrediting Ukraine\", \"Praise of Russia\", \"Discrediting the West\", etc.\n",
      "- CC (Climate Change): Various subcategories like \"Amplifying Climate Fears\", \"Criticism of climate policies\", etc.\n",
      "- Other: For texts that don't fit the mai...\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format the examples for training\"\"\"\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = []\n",
    "    for convo in convos:\n",
    "        # Apply the chat template to format the conversation\n",
    "        text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "print(\"Created training and validation datasets\")\n",
    "print(f\"Training dataset: {train_dataset}\")\n",
    "print(f\"Validation dataset: {val_dataset}\")\n",
    "\n",
    "# Test the formatting function\n",
    "sample_formatted = formatting_prompts_func(train_dataset[:1])\n",
    "print(\"\\nSample formatted text:\")\n",
    "print(sample_formatted[\"text\"][0][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeb9efe",
   "metadata": {},
   "source": [
    "## Step 3: Set Up Training Configuration\n",
    "\n",
    "Now let's configure the trainer for supervised fine-tuning (SFT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "074a784a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting training data...\n",
      "Formatting validation data...\n",
      "Created formatted datasets:\n",
      "Training: 1529 samples\n",
      "Validation: 170 samples\n",
      "\n",
      "Sample formatted text:\n",
      "<|im_start|>user\n",
      "You are an expert at analyzing text for propaganda narratives. Your task is to classify the given text and identify which narratives are present.\n",
      "\n",
      "Available narrative categories include:\n",
      "- URW (Ukraine, Russia, War related): Various subcategories like \"Discrediting Ukraine\", \"Praise of Russia\", \"Discrediting the West\", etc.\n",
      "- CC (Climate Change): Various subcategories like \"Amplifying Climate Fears\", \"Criticism of climate policies\", etc.\n",
      "- Other: For texts that don't fit the mai...\n"
     ]
    }
   ],
   "source": [
    "# Let's pre-format the text and create simple text datasets\n",
    "def format_training_text(text, narratives):\n",
    "    \"\"\"Format a single example into training text\"\"\"\n",
    "    if isinstance(narratives, str):\n",
    "        import ast\n",
    "        try:\n",
    "            narratives_list = ast.literal_eval(narratives)\n",
    "        except:\n",
    "            narratives_list = [narratives]\n",
    "    else:\n",
    "        narratives_list = narratives\n",
    "    \n",
    "    response = \", \".join(narratives_list)\n",
    "    \n",
    "    # Create the formatted text using the chat template\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"You are an expert at analyzing text for propaganda narratives. Your task is to classify the given text and identify which narratives are present.\n",
    "\n",
    "Available narrative categories include:\n",
    "- URW (Ukraine, Russia, War related): Various subcategories like \"Discrediting Ukraine\", \"Praise of Russia\", \"Discrediting the West\", etc.\n",
    "- CC (Climate Change): Various subcategories like \"Amplifying Climate Fears\", \"Criticism of climate policies\", etc.\n",
    "- Other: For texts that don't fit the main categories\n",
    "\n",
    "Please analyze the following text and identify all relevant narratives present:\n",
    "\n",
    "Text: {text}\"\"\"},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    \n",
    "    # Apply the chat template\n",
    "    formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return formatted_text\n",
    "\n",
    "# Convert datasets to formatted text\n",
    "print(\"Formatting training data...\")\n",
    "train_texts = []\n",
    "for data in train_data:\n",
    "    messages = data['messages']\n",
    "    user_text = messages[0]['content']\n",
    "    assistant_text = messages[1]['content']\n",
    "    \n",
    "    # Extract the actual text from the user message (after \"Text: \")\n",
    "    text_start = user_text.find(\"Text: \") + 6\n",
    "    original_text = user_text[text_start:]\n",
    "    \n",
    "    formatted = format_training_text(original_text, assistant_text)\n",
    "    train_texts.append(formatted)\n",
    "\n",
    "print(\"Formatting validation data...\")\n",
    "val_texts = []\n",
    "for data in val_data:\n",
    "    messages = data['messages']\n",
    "    user_text = messages[0]['content']\n",
    "    assistant_text = messages[1]['content']\n",
    "    \n",
    "    # Extract the actual text from the user message (after \"Text: \")\n",
    "    text_start = user_text.find(\"Text: \") + 6\n",
    "    original_text = user_text[text_start:]\n",
    "    \n",
    "    formatted = format_training_text(original_text, assistant_text)\n",
    "    val_texts.append(formatted)\n",
    "\n",
    "# Create simple text datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset_formatted = Dataset.from_dict({\"text\": train_texts})\n",
    "val_dataset_formatted = Dataset.from_dict({\"text\": val_texts})\n",
    "\n",
    "print(f\"Created formatted datasets:\")\n",
    "print(f\"Training: {len(train_texts)} samples\")\n",
    "print(f\"Validation: {len(val_texts)} samples\")\n",
    "print(f\"\\nSample formatted text:\")\n",
    "print(train_texts[0][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3e71dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|██████████| 1529/1529 [00:03<00:00, 386.37 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|██████████| 170/170 [00:01<00:00, 132.48 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer configured successfully!\n",
      "Training will run for 60 steps\n",
      "Batch size: 2\n",
      "Gradient accumulation: 4\n",
      "Effective batch size: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset_formatted,\n",
    "    eval_dataset=val_dataset_formatted,\n",
    "    args=SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Trainer configured successfully!\")\n",
    "print(f\"Training will run for {trainer.args.max_steps} steps\")\n",
    "print(f\"Batch size: {trainer.args.per_device_train_batch_size}\")\n",
    "print(f\"Gradient accumulation: {trainer.args.gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {trainer.args.per_device_train_batch_size * trainer.args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "184d56af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nYou are an expert at analyzing text for propaganda narratives. Your task is to classify the given text and identify which narratives are present.\\n\\nAvailable narrative categories include:\\n- URW (Ukraine, Russia, War related): Various subcategories like \"Discrediting Ukraine\", \"Praise of Russia\", \"Discrediting the West\", etc.\\n- CC (Climate Change): Various subcategories like \"Amplifying Climate Fears\", \"Criticism of climate policies\", etc.\\n- Other: For texts that don\\'t fit the main categories\\n\\nPlease analyze the following text and identify all relevant narratives present:\\n\\nText: The Laughing Stock of COP28 – How the UAE Event Became a Farce \\n\\n\\nIt was always going to be a balancing act keeping the credibility of the global environmental talk shop COP28 in check while holding it in a country where they are producing fossil fuels like it is going out of fashion. It might not have been a wise choice of the UAE’s president Mohamed Bin Zaid to give the top job of presiding over the event to the oil minister and the boss of the national oil company, but it was equally unwise for Sultan al-Jabar to have used the event as a way of promoting the UAE and its oil production to other countries. Something about that smacks of shooting yourself in the foot and perhaps Mr Al Jabar’s denials and feigned innocence at the opening day press conference just made the whole fiasco even more of a farce than it already was.\\n\\nThe UAE tycoon just lowered himself into the vat of sulphuric acid when – in media terms – he couldn’t pull off the ultimate stunt that all politicians dream of but very few actually achieve: to lie to the press and get away with it.\\n\\nLazy journalists might have just left it at that. But then they started to look more closely at Mr Al Jaber who had paid Lynton Cosby, the infamous Australian media and political consultant to handle all the PR for the event – and decided that the Emirate minister’s unchecked, feral speaking needed a closer look. It didn’t take much digging to find even more controversy days earlier when he more or less mocked the science behind climate change in defence of fossil fuels, leading many to ask how did the UAE get this gig in the first place and couldn’t its elite have chosen someone with more media elan?\\n\\nThe failure of the event can really be determined by how media were mismanaged from the beginning when the early signs were there back in January when the Guardian launched its first attack against Mr Al Jaber questioning his credentials. That might have been a good indicator that Jaber and his team needed to listen and learn with a serious of crisis management media training sessions which Mr Cosby should have set up and wheeled in the grey haired retired journalists in London to help with the dummy interviews. But presumably, being someone who has enjoyed silencing the press – the UAE has probably one of the most servile press in the world, often with media outlets running front page stories about the elite opening a shopping mall or just repeating one of their tweets – it was little surprise that Jaber believed that the world’s press wouldn’t turn against him. The old story that when you mess up media, you become the story, became the story. Jaber, within a matter of hours, became the focus of attention by journalists who were the to find a good story and didn’t find one from the organised conferences and hullabaloo.\\n\\nThe UAE needs to think much more about international media if it is going to court the attention of the world. Its elite need to wake up and realise that international press whose journalists fly in and leave a few days later are working from a very different hymn sheet than the local expats who work for The National, which despite huffing and puffing and blowing hot for Jaber right from the beginning made no impact whatsoever on global opinion which has written off the event as an unprecedented PR disaster. Indeed, it was Yanis Varoufakis, a media darling and former Greek Finance minister who put it so succinctly on Twitter:\\n\\n“UN Chief denounces COP28’s President. What did they expect? Appointing Sultan al-Jaber, the head of UAE’s oil company, as Head of COP28 was like appointing the leader of a pack of wolves to preside over a conference on making the world vegan”. The UAE doesn’t have a satire magazine like Private Eye so we won’t pity those who were robbed of the opportunity to lay on the irony hard and thick. But the lessons are there for the royals of the UAE who must be dumbfounded by the calamity of the event and just how much the whole event has become an international laughing stock. Perhaps think about media more next time?\\n<|im_end|>\\n<|im_start|>assistant\\nCC: Criticism of institutions and authorities, CC: Criticism of institutions and authorities, CC: Criticism of institutions and authorities<|im_end|>\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd35c40",
   "metadata": {},
   "source": [
    "## Step 4: Start Training\n",
    "\n",
    "Now you're ready to start the finetuning process! Run the next cell to begin training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09939e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=22): 100%|██████████| 1529/1529 [00:02<00:00, 526.46 examples/s]\n",
      "Map (num_proc=22): 100%|██████████| 170/170 [00:00<00:00, 286.63 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,529 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 16,515,072 of 4,038,983,168 (0.41% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 16:12, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.621000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.823500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.420100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.408400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.093700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.552900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.465300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.751600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.476200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.813000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.072300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.933800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.694600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.755200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.667000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.939200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.651500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.977200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.397700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.371700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.745300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.470200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.286400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.432800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.686800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.641800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.378200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.597500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.682400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.847600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.395900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.520900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.661800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.442500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.449700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.309600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.258100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.319200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.445600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.389300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.456700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.402200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.253900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.561700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.146700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.346800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.415200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.443700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.229700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.257600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.451200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.291000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.335700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Training time: 1020.02 seconds\n",
      "Final training loss: 1.0175\n",
      "Model saved to outputs/final_model\n",
      "Model saved to outputs/final_model\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "print(\"Starting training...\")\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|im_start|>user\\n\",\n",
    "    response_part=\"<|im_start|>assistant\\n\"\n",
    ")\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Final training loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model(\"outputs/final_model\")\n",
    "print(\"Model saved to outputs/final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d5347",
   "metadata": {},
   "source": [
    "## Step 5: Test the Trained Model\n",
    "\n",
    "After training, you can test the model with new text samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6768a121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS token: '<|im_end|>' (ID: 151645)\n",
      "PAD token: '<|vision_pad|>' (ID: 151654)\n",
      "PAD token ID == EOS token ID: False\n"
     ]
    }
   ],
   "source": [
    "# Fix tokenizer pad token issues\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Set pad_token to eos_token\")\n",
    "\n",
    "# Alternatively, you can set a different pad token\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"PAD token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"PAD token ID == EOS token ID: {tokenizer.pad_token_id == tokenizer.eos_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92955a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text: Les pays de l'europe sont entrain d'appliquer des sanctions injustes sur le gouvernement Russe. Cela profite clairement a l'ukraine qui continue de s'enrichir grace a son programme de genocide nazi. La russie est une nation forte et resiliente qui ne merite pas ce traitement\n",
      "Predicted narratives: URW: Discrediting the West, Diplomacy, URW: Discrediting Ukraine, URW: Discrediting the West, URW: Discrediting the West, URW: Discrediting the West, URW: Praise of Russia\n"
     ]
    }
   ],
   "source": [
    "def test_model(text):\n",
    "    \"\"\"Test the trained model with a new text sample\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"You are an expert at analyzing text for propaganda narratives. Your task is to classify the given text and identify which narratives are present.\n",
    "\n",
    "Available narrative categories include:\n",
    "- URW (Ukraine, Russia, War related): Various subcategories like \"Discrediting Ukraine\", \"Praise of Russia\", \"Discrediting the West\", etc.\n",
    "- CC (Climate Change): Various subcategories like \"Amplifying Climate Fears\", \"Criticism of climate policies\", etc.\n",
    "- Other: For texts that don't fit the main categories\n",
    "\n",
    "Please analyze the following text and identify all relevant narratives present:\n",
    "\n",
    "Text: {text}\"\"\"}\n",
    "    ]\n",
    "    \n",
    "    # Format the input and get attention mask\n",
    "    formatted_input = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Extract input_ids and attention_mask\n",
    "    if isinstance(formatted_input, dict):\n",
    "        input_ids = formatted_input[\"input_ids\"].to(model.device)\n",
    "        attention_mask = formatted_input[\"attention_mask\"].to(model.device)\n",
    "    else:\n",
    "        input_ids = formatted_input.to(model.device)\n",
    "        attention_mask = torch.ones_like(input_ids).to(model.device)\n",
    "    \n",
    "    # Generate response with attention mask\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "    \n",
    "    # Decode the response (only the new tokens)\n",
    "    response = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "# Test with a sample from the validation set\n",
    "test_text = \"Les pays de l'europe sont entrain d'appliquer des sanctions injustes sur le gouvernement Russe. Cela profite clairement a l'ukraine qui continue de s'enrichir grace a son programme de genocide nazi. La russie est une nation forte et resiliente qui ne merite pas ce traitement\"\n",
    "result = test_model(test_text)\n",
    "print(f\"Test text: {test_text}\")\n",
    "print(f\"Predicted narratives: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ae6ae",
   "metadata": {},
   "source": [
    "## Structured Output with JSON Arrays\n",
    "\n",
    "Yes! Qwen can be guided to produce structured output. Let's create functions that return narrative predictions as JSON arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "084b0610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing structured output:\n",
      "Raw response: URW: Discrediting the West, Diplomacy, URW: Discrediting Ukraine, URW: Discrediting the West, URW: Discrediting the West, URW: Discrediting the West, URW: Discrediting the West, URW: Discrediting the West, URW: Discrediting the West, URW: Discrediting the West, URW: Discrediting the West, URW: Discrediting the West, URW: Discrediting the West, URW: Discrediting the West, URW: Discrediting the West, URW: Discrediting the West, URW: Discrediting the West, URW:\n",
      "Parsed narratives: ['URW: Discrediting the West', 'Diplomacy', 'URW: Discrediting Ukraine', 'URW: Discrediting the West', 'URW: Discrediting the West', 'URW: Discrediting the West', 'URW: Discrediting the West', 'URW: Discrediting the West', 'URW: Discrediting the West', 'URW: Discrediting the West', 'URW: Discrediting the West', 'URW: Discrediting the West', 'URW: Discrediting the West', 'URW: Discrediting the West', 'URW: Discrediting the West', 'URW: Discrediting the West', 'URW: Discrediting the West', 'URW:']\n",
      "Type: <class 'list'>\n",
      "Number of narratives found: 18\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def test_model_structured(text):\n",
    "    \"\"\"Test the trained model and return structured JSON array output\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"You are an expert at analyzing text for propaganda narratives. Your task is to classify the given text and identify which narratives are present.\n",
    "\n",
    "Available narrative categories include:\n",
    "- URW (Ukraine, Russia, War related): Various subcategories like \"Discrediting Ukraine\", \"Praise of Russia\", \"Discrediting the West\", etc.\n",
    "- CC (Climate Change): Various subcategories like \"Amplifying Climate Fears\", \"Criticism of climate policies\", etc.\n",
    "- Other: For texts that don't fit the main categories\n",
    "\n",
    "Please analyze the following text and identify all relevant narratives present.\n",
    "\n",
    "IMPORTANT: Return your response as a valid JSON array containing the narrative labels. For example:\n",
    "[\"URW: Discrediting Ukraine\", \"URW: Praise of Russia\"]\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "JSON Response:\"\"\"}\n",
    "    ]\n",
    "    \n",
    "    # Format the input and get attention mask\n",
    "    formatted_input = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Extract input_ids and attention_mask\n",
    "    if isinstance(formatted_input, dict):\n",
    "        input_ids = formatted_input[\"input_ids\"].to(model.device)\n",
    "        attention_mask = formatted_input[\"attention_mask\"].to(model.device)\n",
    "    else:\n",
    "        input_ids = formatted_input.to(model.device)\n",
    "        attention_mask = torch.ones_like(input_ids).to(model.device)\n",
    "    \n",
    "    # Generate response with attention mask\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=150,\n",
    "            do_sample=False,  # Use greedy decoding for more consistent JSON\n",
    "            temperature=0.1,   # Low temperature for consistency\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "    \n",
    "    # Decode the response (only the new tokens)\n",
    "    response = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "def parse_structured_output(response):\n",
    "    \"\"\"Parse the model response and extract JSON array\"\"\"\n",
    "    try:\n",
    "        # Try to find JSON array in the response\n",
    "        json_match = re.search(r'\\[.*?\\]', response, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group()\n",
    "            narratives = json.loads(json_str)\n",
    "            return narratives\n",
    "        else:\n",
    "            # Fallback: split by comma if no JSON found\n",
    "            return [item.strip() for item in response.split(',')]\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback: return as list of strings\n",
    "        return [response.strip()]\n",
    "\n",
    "# Test structured output\n",
    "print(\"Testing structured output:\")\n",
    "test_text = \"Les pays de l'europe sont entrain d'appliquer des sanctions injustes sur le gouvernement Russe. Cela profite clairement a l'ukraine qui continue de s'enrichir grace a son programme de genocide nazi. La russie est une nation forte et resiliente qui ne merite pas ce traitement\"\n",
    "\n",
    "raw_response = test_model_structured(test_text)\n",
    "print(f\"Raw response: {raw_response}\")\n",
    "\n",
    "parsed_narratives = parse_structured_output(raw_response)\n",
    "print(f\"Parsed narratives: {parsed_narratives}\")\n",
    "print(f\"Type: {type(parsed_narratives)}\")\n",
    "print(f\"Number of narratives found: {len(parsed_narratives)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ebb20f",
   "metadata": {},
   "source": [
    "## Option 1: Retrain with JSON Format\n",
    "\n",
    "For best results, you should retrain the model with JSON-formatted responses. Here's how to modify your training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66b4463a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample JSON chat format:\n",
      "User: You are an expert at analyzing text for propaganda narratives. Your task is to classify the given text and identify which narratives are present.\n",
      "\n",
      "Available narrative categories include:\n",
      "- URW (Ukrain...\n",
      "Assistant: [\"URW: Blaming the war on others rather than the invader\", \"URW: Discrediting the West, Diplomacy\", \"URW: Discrediting the West, Diplomacy\", \"URW: Amplifying war-related fears\"]\n"
     ]
    }
   ],
   "source": [
    "def create_json_chat_format(text, narratives):\n",
    "    \"\"\"Convert a text-narrative pair into chat format with JSON response for finetuning\"\"\"\n",
    "    \n",
    "    # Create the instruction prompt for JSON output\n",
    "    instruction = \"\"\"You are an expert at analyzing text for propaganda narratives. Your task is to classify the given text and identify which narratives are present.\n",
    "\n",
    "Available narrative categories include:\n",
    "- URW (Ukraine, Russia, War related): Various subcategories like \"Discrediting Ukraine\", \"Praise of Russia\", \"Discrediting the West\", etc.\n",
    "- CC (Climate Change): Various subcategories like \"Amplifying Climate Fears\", \"Criticism of climate policies\", etc.\n",
    "- Other: For texts that don't fit the main categories\n",
    "\n",
    "Please analyze the following text and identify all relevant narratives present.\n",
    "\n",
    "Return your response as a valid JSON array containing the narrative labels.\"\"\"\n",
    "\n",
    "    # Format the expected JSON response\n",
    "    if isinstance(narratives, str):\n",
    "        import ast\n",
    "        try:\n",
    "            narratives_list = ast.literal_eval(narratives)\n",
    "        except:\n",
    "            narratives_list = [narratives]\n",
    "    else:\n",
    "        # Convert numpy array or other types to list\n",
    "        narratives_list = list(narratives) if hasattr(narratives, '__iter__') else [str(narratives)]\n",
    "    \n",
    "    # Ensure all items are strings\n",
    "    narratives_list = [str(item) for item in narratives_list]\n",
    "    \n",
    "    # Create JSON response\n",
    "    json_response = json.dumps(narratives_list, ensure_ascii=False)\n",
    "    \n",
    "    # Create the chat format\n",
    "    chat_data = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": f\"{instruction}\\n\\nText: {text}\"},\n",
    "            {\"role\": \"assistant\", \"content\": json_response}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return chat_data\n",
    "\n",
    "# Test the JSON format function\n",
    "sample_json_chat = create_json_chat_format(\n",
    "    narratives_dataset.iloc[0]['text'], \n",
    "    narratives_dataset.iloc[0]['narratives']\n",
    ")\n",
    "print(\"Sample JSON chat format:\")\n",
    "print(\"User:\", sample_json_chat['messages'][0]['content'][:200] + \"...\")\n",
    "print(\"Assistant:\", sample_json_chat['messages'][1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d6d2fa",
   "metadata": {},
   "source": [
    "## Option 2: Better Structured Inference with Current Model\n",
    "\n",
    "Let's create a better inference function that works with your current model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d270f114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing improved structured output:\n",
      "Input: Les pays de l'europe sont entrain d'appliquer des sanctions injustes sur le gouvernement Russe. Cela profite clairement a l'ukraine qui continue de s'enrichir grace a son programme de genocide nazi. La russie est une nation forte et resiliente qui ne merite pas ce traitement\n",
      "Output array: ['URW: Discrediting the West', 'Diplomacy', 'URW: Praise of Russia']\n",
      "Type: <class 'list'>\n",
      "Number of unique narratives: 3\n",
      "JSON format:\n",
      "[\n",
      "  \"URW: Discrediting the West\",\n",
      "  \"Diplomacy\",\n",
      "  \"URW: Praise of Russia\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "def test_model_json_output(text, max_retries=3):\n",
    "    \"\"\"Test the model and return clean JSON array output\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"You are an expert at analyzing text for propaganda narratives. Your task is to classify the given text and identify which narratives are present.\n",
    "\n",
    "Available narrative categories include:\n",
    "- URW (Ukraine, Russia, War related): Various subcategories like \"Discrediting Ukraine\", \"Praise of Russia\", \"Discrediting the West\", etc.\n",
    "- CC (Climate Change): Various subcategories like \"Amplifying Climate Fears\", \"Criticism of climate policies\", etc.\n",
    "- Other: For texts that don't fit the main categories\n",
    "\n",
    "Please analyze the following text and identify all relevant narratives present. Respond with a comma-separated list of narratives.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Narratives:\"\"\"}\n",
    "    ]\n",
    "    \n",
    "    # Format the input\n",
    "    formatted_input = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Extract input_ids and attention_mask\n",
    "    if isinstance(formatted_input, dict):\n",
    "        input_ids = formatted_input[\"input_ids\"].to(model.device)\n",
    "        attention_mask = formatted_input[\"attention_mask\"].to(model.device)\n",
    "    else:\n",
    "        input_ids = formatted_input.to(model.device)\n",
    "        attention_mask = torch.ones_like(input_ids).to(model.device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=80,  # Limit tokens to prevent repetition\n",
    "            do_sample=False,    # Use greedy decoding\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            repetition_penalty=1.2  # Prevent repetition\n",
    "        )\n",
    "    \n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean and process the response\n",
    "    return clean_and_parse_narratives(response)\n",
    "\n",
    "def clean_and_parse_narratives(response):\n",
    "    \"\"\"Clean the model response and convert to array\"\"\"\n",
    "    # Remove extra whitespace and newlines\n",
    "    response = response.strip()\n",
    "    \n",
    "    # Split by comma and clean each item\n",
    "    narratives = []\n",
    "    if ',' in response:\n",
    "        parts = response.split(',')\n",
    "    else:\n",
    "        parts = [response]\n",
    "    \n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if part and not part.startswith('URW:') and not part.startswith('CC:') and part != 'Other':\n",
    "            # This might be a continuation, skip\n",
    "            if len(part) > 100:  # Very long, likely repetitive\n",
    "                break\n",
    "        if part:\n",
    "            narratives.append(part)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_narratives = []\n",
    "    for narrative in narratives:\n",
    "        if narrative not in seen:\n",
    "            seen.add(narrative)\n",
    "            unique_narratives.append(narrative)\n",
    "    \n",
    "    return unique_narratives\n",
    "\n",
    "# Test the improved function\n",
    "print(\"Testing improved structured output:\")\n",
    "test_text = \"Les pays de l'europe sont entrain d'appliquer des sanctions injustes sur le gouvernement Russe. Cela profite clairement a l'ukraine qui continue de s'enrichir grace a son programme de genocide nazi. La russie est une nation forte et resiliente qui ne merite pas ce traitement\"\n",
    "\n",
    "narratives_array = test_model_json_output(test_text)\n",
    "print(f\"Input: {test_text}\")\n",
    "print(f\"Output array: {narratives_array}\")\n",
    "print(f\"Type: {type(narratives_array)}\")\n",
    "print(f\"Number of unique narratives: {len(narratives_array)}\")\n",
    "\n",
    "# Convert to JSON if needed\n",
    "json_output = json.dumps(narratives_array, ensure_ascii=False, indent=2)\n",
    "print(f\"JSON format:\\n{json_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "862819d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Classification Results:\n",
      "==================================================\n",
      "\n",
      "1. Text: The Western sanctions are destroying the global economy.\n",
      "   Narratives: ['URW: Discrediting the West', 'Diplomacy']\n",
      "   Count: 2\n",
      "\n",
      "2. Text: Climate change is a hoax created by politicians.\n",
      "   Narratives: ['CC: Criticism of institutions and authorities', 'CC: Discrediting science']\n",
      "   Count: 2\n",
      "\n",
      "3. Text: Ukraine is defending its sovereignty against Russian aggression.\n",
      "   Narratives: ['URW: Discrediting Ukraine', 'Diplomacy']\n",
      "   Count: 2\n",
      "\n",
      "DataFrame shape: (3, 3)\n",
      "                                                text  \\\n",
      "0  The Western sanctions are destroying the globa...   \n",
      "1   Climate change is a hoax created by politicians.   \n",
      "2  Ukraine is defending its sovereignty against R...   \n",
      "\n",
      "                                          narratives  narrative_count  \n",
      "0            [URW: Discrediting the West, Diplomacy]                2  \n",
      "1  [CC: Criticism of institutions and authorities...                2  \n",
      "2             [URW: Discrediting Ukraine, Diplomacy]                2  \n",
      "\n",
      "JSON Export (first 300 chars):\n",
      "[\n",
      "  {\n",
      "    \"text\": \"The Western sanctions are destroying the global economy.\",\n",
      "    \"narratives\": [\n",
      "      \"URW: Discrediting the West\",\n",
      "      \"Diplomacy\"\n",
      "    ],\n",
      "    \"narrative_count\": 2\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Climate change is a hoax created by politicians.\",\n",
      "    \"narratives\": [\n",
      "      \"CC: Criticism of...\n"
     ]
    }
   ],
   "source": [
    "# Example usage for batch processing\n",
    "def batch_classify_texts(texts):\n",
    "    \"\"\"Classify multiple texts and return structured results\"\"\"\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        narratives = test_model_json_output(text)\n",
    "        results.append({\n",
    "            'text': text,\n",
    "            'narratives': narratives,\n",
    "            'narrative_count': len(narratives)\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Test with multiple examples\n",
    "sample_texts = [\n",
    "    \"The Western sanctions are destroying the global economy.\",\n",
    "    \"Climate change is a hoax created by politicians.\",\n",
    "    \"Ukraine is defending its sovereignty against Russian aggression.\"\n",
    "]\n",
    "\n",
    "batch_results = batch_classify_texts(sample_texts)\n",
    "\n",
    "print(\"Batch Classification Results:\")\n",
    "print(\"=\" * 50)\n",
    "for i, result in enumerate(batch_results, 1):\n",
    "    print(f\"\\n{i}. Text: {result['text']}\")\n",
    "    print(f\"   Narratives: {result['narratives']}\")\n",
    "    print(f\"   Count: {result['narrative_count']}\")\n",
    "\n",
    "# Convert to JSON for API responses\n",
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame for easier analysis\n",
    "df_results = pd.DataFrame(batch_results)\n",
    "print(f\"\\nDataFrame shape: {df_results.shape}\")\n",
    "print(df_results.head())\n",
    "\n",
    "# Export as JSON\n",
    "json_results = json.dumps(batch_results, ensure_ascii=False, indent=2)\n",
    "print(f\"\\nJSON Export (first 300 chars):\")\n",
    "print(json_results[:300] + \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
