{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2538270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "# Set the current working directory to the project root\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21746730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NARRATIVE_THRESHOLD = 0.89\n",
    "SUBNARRATIVE_THRESHOLD = 0.80\n",
    "BEST_MODEL_CHECKPOINT_PATH = 'models/phase0_xlmr_best_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1b5a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.data_preparation import prepare_dataframes\n",
    "\n",
    "# Define constants\n",
    "DATA_FOLDER = 'data'\n",
    "\n",
    "# Prepare the dataframes\n",
    "(\n",
    "    train_df,\n",
    "    val_df,\n",
    "    test_df,\n",
    "    id_to_label,\n",
    "    label_to_id,\n",
    "    parent_child_pairs,\n",
    ") = prepare_dataframes(data_folder=DATA_FOLDER)\n",
    "\n",
    "num_total_labels = len(id_to_label)\n",
    "\n",
    "print(f\"Number of training examples: {len(train_df)}\")\n",
    "print(f\"Number of validation examples: {len(val_df)}\")\n",
    "print(f\"Number of testing examples: {len(test_df)}\")\n",
    "print(f\"Number of labels: {num_total_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb32c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Analyze Class Distribution on the Training Set ---\n",
    "from src.utils.metrics import get_class_distribution\n",
    "\n",
    "\n",
    "print(\"--- Analyzing Training Set Class Distribution ---\")\n",
    "# Use the reusable function to get the counts\n",
    "train_class_distribution = get_class_distribution(train_df, id_to_label)\n",
    "# Display the rarest classes, which are your primary candidates for augmentation\n",
    "print(\"\\nTop 20 Rarest Classes in Training Data:\")\n",
    "print(train_class_distribution.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab0c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from src.data_management.datasets import NarrativeClassificationDataset\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = 'xlm-roberta-base'\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "print(\"Creating validation dataset and dataloader...\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_total_labels,\n",
    "    problem_type='multi_label_classification',\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(BEST_MODEL_CHECKPOINT_PATH))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7399d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "val_dataset = NarrativeClassificationDataset(val_df, tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Validation dataset created with {len(val_dataset)} examples.\")\n",
    "print(f\"Validation dataloader created with batch size {BATCH_SIZE}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d459bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Analyze Per-Class F1 Scores on the Validation Set ---\n",
    "from src.training.engine import get_raw_predictions, compute_metrics\n",
    "\n",
    "\n",
    "print(\"\\n--- Analyzing Per-Class F1 Scores on Validation Set ---\")\n",
    "# First, get the predictions and true labels from your validation set\n",
    "val_logits, val_true_labels = get_raw_predictions(model, val_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.utils.metrics import get_per_class_f1_scores\n",
    "\n",
    "# --- Step 5: Get Per-Class F1 Scores with Optimal Thresholds ---\n",
    "print(\"\\n--- Calculating Per-Class F1 Scores with Best Thresholds ---\")\n",
    "\n",
    "# We need to identify which columns in our label tensors correspond to narratives vs. sub-narratives\n",
    "narrative_indices = [i for i, label in id_to_label.items() if label.count(':') == 1]\n",
    "subnarrative_indices = [i for i, label in id_to_label.items() if label.count(':') == 2]\n",
    "\n",
    "# Use the dedicated function to get the F1 scores per class\n",
    "per_class_f1_df = get_per_class_f1_scores(\n",
    "    true_labels=val_true_labels,\n",
    "    pred_logits=val_logits,\n",
    "    id_to_label_map=id_to_label,\n",
    "    narrative_indices=narrative_indices,\n",
    "    subnarrative_indices=subnarrative_indices,\n",
    "    narrative_threshold=NARRATIVE_THRESHOLD, # Using the constant from the top of the notebook\n",
    "    subnarrative_threshold=SUBNARRATIVE_THRESHOLD # Using the constant from the top of the notebook\n",
    ")\n",
    "\n",
    "print(\"\\nPer-Class F1 Scores (sorted by F1 score):\")\n",
    "# Display the full dataframe to see all classes\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(per_class_f1_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45c4ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Identify Least Performing Narratives and Subnarratives (Bottom 30%) ---\n",
    "\n",
    "# Separate narratives and subnarratives by label format\n",
    "narratives_df = per_class_f1_df[per_class_f1_df['label'].apply(lambda x: x.count(':') == 1)]\n",
    "subnarratives_df = per_class_f1_df[per_class_f1_df['label'].apply(lambda x: x.count(':') == 2)]\n",
    "\n",
    "# Calculate bottom 30% count for each\n",
    "narr_bottom_n = max(1, int(len(narratives_df) * 0.3))\n",
    "subnarr_bottom_n = max(1, int(len(subnarratives_df) * 0.3))\n",
    "\n",
    "least_perf_narratives = narratives_df.nsmallest(narr_bottom_n, 'f1_score')\n",
    "least_perf_subnarratives = subnarratives_df.nsmallest(subnarr_bottom_n, 'f1_score')\n",
    "\n",
    "print(\"\\n--- Least Performing Narratives (Bottom 30%) ---\")\n",
    "print(least_perf_narratives[['label', 'f1_score']])\n",
    "\n",
    "print(\"\\n--- Least Performing Subnarratives (Bottom 30%) ---\")\n",
    "print(least_perf_subnarratives[['label', 'f1_score']])\n",
    "\n",
    "# Save as CSV for easy inspection and re-use\n",
    "least_perf_narratives.to_csv('least_perf_narratives.csv', index=False)\n",
    "least_perf_subnarratives.to_csv('least_perf_subnarratives.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a4324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
